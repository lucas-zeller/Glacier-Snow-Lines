{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4632c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "folder_debris = os.path.join(folder_AGVA, 'debris cover', 'raster')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3031\n"
     ]
    }
   ],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 0\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# open list of validation glaciers\n",
    "all_validation_df = pd.read_csv(os.path.join(folder_AGVA, 'Validation', 'Validation Glaciers.csv'))\n",
    "\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# select which rgis to analyze\n",
    "if validation_only:\n",
    "    rgis_to_analyze = list( set(rgis_folder).intersection(set(all_validation_df['RGIId'].values)) )\n",
    "else:\n",
    "    # rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "    rgis_to_analyze = rgis_folder # everything that is available\n",
    "#     rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# get list of glacier area for each rgi\n",
    "areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# make df\n",
    "rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "\n",
    "# sort however you want\n",
    "rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# grab rgi names\n",
    "rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "print(len(rgis_to_analyze_df))\n",
    "# print(rgis_to_analyze[:10])\n",
    "# print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d78c89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3030 of 3031: RGI60-01.13635  3025.115 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "\n",
      "Starting 3031 of 3031: RGI60-01.13696  3362.656 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "#     if c>0: continue\n",
    "#     if i<43: continue\n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "#     if rgiid!= \"RGI60-01.15731\": continue #10910 08989\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "\n",
    "#     if ga<332: continue\n",
    "    if ga<3000: continue\n",
    "        \n",
    "    # set folder\n",
    "    if validation_only:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation')\n",
    "    else:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "       \n",
    "    # check if this glacier has been run already, skip if so\n",
    "    temp_path = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_2022_daily_AAs_shadowed.nc\")\n",
    "    if os.path.exists(temp_path): continue\n",
    "        \n",
    "    # check if the initial products we need exist yet. if not, then continue\n",
    "    path_1 = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_2022_daily_AAs.nc\")\n",
    "    path_2 = os.path.join(folder_save, 'Shadows', f\"S2_{rgiid}_2022_daily_shadows.nc\")\n",
    "    if not os.path.exists(path_1): continue\n",
    "    if not os.path.exists(path_2): continue\n",
    "    \n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\")\n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    " \n",
    "    # open glacier mask, count how many pixels there are\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto').glacier\n",
    "    \n",
    "    # open the observed faction df\n",
    "#     obs_path = os.path.join(folder_save, 'Daily AAs', 'observed', f\"S2_{rgiid}_observed.csv\")\n",
    "#     obs_df = pd.read_csv(obs_path)\n",
    "    \n",
    "    # for each year, open the daily data, coarsen and resave\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        if ga>100: print(y)\n",
    "#         if y!=2018: continue\n",
    "        \n",
    "        # open data\n",
    "        path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "        path_shadow = os.path.join(folder_save, 'Shadows', f\"S2_{rgiid}_{y}_daily_shadows.nc\")\n",
    "        path_debris = os.path.join(folder_debris, f\"{rgiid}_debris.tif\")\n",
    "        \n",
    "        # if small glacier, we dont need to chunk\n",
    "        if ga>1000:\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':1})\n",
    "            shadows = xr.open_dataset(path_shadow, chunks={'time':1})\n",
    "            debris = riox.open_rasterio(path_debris)\n",
    "        elif ga>100:\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':10})\n",
    "            shadows = xr.open_dataset(path_shadow, chunks={'time':10})\n",
    "            debris = riox.open_rasterio(path_debris)\n",
    "        else:\n",
    "            snow = xr.open_dataset(path_open)\n",
    "            shadows = xr.open_dataset(path_shadow)\n",
    "            debris = riox.open_rasterio(path_debris)\n",
    "        \n",
    "        # at this point:\n",
    "        # snow=0 is off-glacier, shadow, cloud\n",
    "        # snow=1 is snow\n",
    "        # snow=2,3,4,6 is firn,ice,debris,water (in that order) (5 is shadow)\n",
    "        # terrain shadow mask has 1=usable, 0=shadow or off-glacier\n",
    "        # debris mask has 1=debris, 0=not debris\n",
    "        \n",
    "        # apply debris mask. where 'snow' is not in terrain shadow and is classified as shadow, ice, or firn, but should be debris\n",
    "        # make snow 4 where (debris=1) and (shadow=1) and (snow=0 or 2 or 3)\n",
    "        snow = xr.where( (debris==1) & (shadows==1) & (snow.isin([2,3,5])), 4, snow)\n",
    "        \n",
    "#         # apply terrain shadow mask\n",
    "#         # make snow 0 where shadows=0\n",
    "        snow = snow.where(shadows==1, 0)\n",
    "        \n",
    "#         # make all shadows 0\n",
    "        snow = snow.where(snow!=5,0)\n",
    "\n",
    "#         # reapply mask\n",
    "        snow = snow.where(glacier_mask>0.5,0).astype('uint8')\n",
    "     \n",
    "        # save\n",
    "        path_temp = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed.nc\")\n",
    "        encoding = {\"class\":{\"zlib\": True, \"dtype\": \"uint8\"}}\n",
    "        snow.to_netcdf(path_temp, encoding=encoding)\n",
    "     \n",
    "    c+=1\n",
    "print(\"Done!\")\n",
    "# should start at 1953, 6.217 km2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
