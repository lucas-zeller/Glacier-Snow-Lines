{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82230f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3031\n"
     ]
    }
   ],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 0\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# open list of validation glaciers\n",
    "all_validation_df = pd.read_csv(os.path.join(folder_AGVA, 'Validation', 'Validation Glaciers.csv'))\n",
    "\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# select which rgis to analyze\n",
    "if validation_only:\n",
    "    rgis_to_analyze = list( set(rgis_folder).intersection(set(all_validation_df['RGIId'].values)) )\n",
    "else:\n",
    "    # rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "    rgis_to_analyze = rgis_folder # everything that is available\n",
    "#     rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# get list of glacier area for each rgi\n",
    "areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# make df\n",
    "rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "\n",
    "# sort however you want\n",
    "rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# grab rgi names\n",
    "rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "\n",
    "print(len(rgis_to_analyze_df))\n",
    "# print(rgis_to_analyze[:10])\n",
    "# print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d78c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 3023 of 3031: RGI60-01.17614  925.341 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "\n",
    "    # options for skipping\n",
    "#     if (i+1)<296: continue #everything done to here\n",
    "#     if (i+1)>400: continue #everything done to here \n",
    "#     if (i+1) not in [405,406,407,408,420, 291]: continue\n",
    "#     if (i+1) not in [280, 290]: continue\n",
    "#     if rgiid != \"RGI60-01.03215\": continue\n",
    "#     if skip: continue\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "    if ga<925: continue\n",
    "    \n",
    "    # print progress\n",
    "#     if (i+1)!=4: continue\n",
    "#     if (i+1)!=11: continue\n",
    "#     if (i+1)>1: continue\n",
    "    \n",
    "\n",
    "    if validation_only:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation')\n",
    "    else:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "        \n",
    "    # check if this glacier has been run already, skip if so\n",
    "    temp_path = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_2022_daily_AAs.nc\")\n",
    "#     if os.path.exists(temp_path):\n",
    "#         continue\n",
    "        \n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\")\n",
    "        \n",
    "#     break\n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    "    \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto').glacier \n",
    "    \n",
    "    # open the classification data. only chunk (use dask) if the glacier is \"big\"\n",
    "    file_name = f\"S2_{rgiid}_2018-01-01_2023-01-01\"\n",
    "    if ga>500:\n",
    "        xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\"), chunks={'band':1})#.chunk({'band':-1, 'y':'auto', 'x':'auto'})\n",
    "    elif ga>100:\n",
    "        xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\"), chunks={'band':10})#.chunk({'band':-1, 'y':'auto', 'x':'auto'})\n",
    "    else:\n",
    "        xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\"))\n",
    "        \n",
    "    # load metadata csv, convert date to datetimes\n",
    "    meta_fp = os.path.join(folder_meta, f\"{file_name}.csv\")\n",
    "    meta_df = pd.read_csv(meta_fp)\n",
    "    \n",
    "    # format time axis to pandas datetime, like xarray wants\n",
    "    datetimes = pd.to_datetime([f\"{str(i)[:4]}-{str(i)[4:6]}-{str(i)[6:]}\" for i in meta_df['date']])\n",
    "    datetimes_unique = datetimes.unique()\n",
    "    xr_class = xr_class.rename({\"band\":\"time\"})\n",
    "    xr_class['time'] = datetimes\n",
    "\n",
    "    # merge images on same day, if there are repeated dates\n",
    "    if len(datetimes)!=len(datetimes.unique()):\n",
    "        xr_class = xr_class.where(xr_class<20, 0).groupby('time').max('time')\n",
    "    else:\n",
    "        xr_class = xr_class.where(xr_class<20, 0)\n",
    "\n",
    "    # get these merged dates\n",
    "    datetimes_merged = xr_class.time.values\n",
    "\n",
    "    ### create binary mask of useable and unuseable data, use it to mask the xr_class to only this area\n",
    "    bad_classes = [0] #class 5 is shadow. we will call these area unusable\n",
    "    good_classes = [1,2,3,4,5,6] #snow,firn,ice,debris,water are usable areas\n",
    "    # usable = xr.where( xr_class.isin(good_classes), 1, 0)\n",
    "\n",
    "    # count total number of pixels on the glacier surface, based on the glacier rgi area\n",
    "#     glacier_pixels = int(ga * (1000*1000) / (10*10))\n",
    "    glacier_pixels = glacier_mask.sum().values\n",
    "#     print('here2')\n",
    "    \n",
    "    # now we can mask out unusable areas in each time step\n",
    "    xr_class = xr.where(xr_class.isin(good_classes), xr_class, 0) #.sel(time=good_times) # note we subset to only the good days\n",
    "\n",
    "    # make df with zeros that can be filled in later\n",
    "    obs_df = pd.DataFrame({'Date':datetimes_merged, 'observed_initial':[0 for i in datetimes_merged]})\n",
    "\n",
    "    # at this point, xr_class is 0 off-glacier, 0 in shadow/cloud, and 1-6 in usable areas\n",
    "    # save one year at a time\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        if ga>200: print(y)\n",
    "#         if y==2018: continue\n",
    "#         if y<2020: continue\n",
    "        path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "        \n",
    "        # new method we only use months 5-11\n",
    "        save_xr = xr_class.sel(time=slice(f\"{y}-5-01\", f\"{y}-11-30\")).astype('uint8').rename('class')\n",
    "\n",
    "        # specify compression/encoding\n",
    "        encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "        # save\n",
    "        save_xr.to_netcdf(path_save, encoding=encoding)\n",
    "        \n",
    "#         # get dates\n",
    "#         dates_this_year = save_xr.time.values\n",
    "        \n",
    "#         ### then calculate % useable on each date, add to csv to save\n",
    "#         # if this is a big glacier, then lets reopen to calculate this\n",
    "#         # if REALLY big, then coarsen before computing\n",
    "#         if ga>3000:\n",
    "#             continue\n",
    "            \n",
    "#         elif ga>500:                   \n",
    "#             # open data\n",
    "#             xr_to_use = xr.open_dataset(path_save, chunks={'band':1, 'y':-1, 'x':-1})['class']\n",
    "\n",
    "#             # coarsen and save\n",
    "#             path_temp = os.path.join(folder_save, 'temp.tif')\n",
    "#             usable = xr.where(xr_to_use>0, 1, 0).astype(\"uint8\").coarsen({\"x\":5, \"y\":5}, boundary=\"trim\").median(skipna=True).astype(\"uint8\").compute()\n",
    "#             usable.rio.to_raster(path_temp, dtype='uint8')\n",
    "            \n",
    "#             # reopen and calculate percent useable on each day\n",
    "#             usable = riox.open_rasterio(path_temp)\n",
    "#             percent_usable_by_time = (usable.sum(dim=['x','y'])).compute().values\n",
    "            \n",
    "#             # scale correctly\n",
    "#             percent_usable_by_time = percent_usable_by_time*25/glacier_pixels\n",
    "        \n",
    "#         elif ga>150:\n",
    "#             xr_to_use = xr.open_dataset(path_save, chunks={'band':1, 'y':-1, 'x':-1}).to_array()\n",
    "#             percent_usable_by_time = (xr.where(xr_to_use>0, 1, 0).sum(dim=['x','y'])/glacier_pixels).compute().values[0]\n",
    "        \n",
    "#         else:\n",
    "#             xr_to_use = save_xr\n",
    "#             percent_usable_by_time = (xr.where(xr_to_use>0, 1, 0).sum(dim=['x','y'])/glacier_pixels).compute().values\n",
    "        \n",
    "        \n",
    "        \n",
    "# #         print(obs_df['Date'].isin(dates_this_year).sum())\n",
    "# #         print(percent_usable_by_time)\n",
    "        \n",
    "#         obs_df.loc[obs_df['Date'].isin(dates_this_year), 'observed_initial'] = percent_usable_by_time.round(4)\n",
    "    \n",
    "#     if ga<3000:\n",
    "#         out_path_csv = os.path.join(folder_save, 'Daily AAs', 'observed', f\"S2_{rgiid}_observed.csv\")\n",
    "#         obs_df.to_csv(out_path_csv, index=False)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4555f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_class=0\n",
    "xr_to_use = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
