{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed27064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi, drop='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f735687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 1\n",
    "\n",
    "# open list of validation glaciers\n",
    "all_validation_df = pd.read_csv(os.path.join(folder_AGVA, 'Validation', 'Validation Glaciers.csv'))\n",
    "\n",
    "### get list of all the glaciers for which we have calculated the snow covered fractions\n",
    "# select which rgis to analyze\n",
    "if validation_only:\n",
    "    folder_sca = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation', 'Band SCFs')\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation')\n",
    "else:\n",
    "    folder_sca = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Band SCFs')\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_to_analyze = list(set( [ i[3:17] for i in os.listdir(folder_sca) if i[-3:]=='csv' ] ))\n",
    "# rgis_to_analyze.sort()\n",
    "\n",
    "# get list of glacier area for each rgi\n",
    "areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# make df\n",
    "rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "\n",
    "# sort however you want\n",
    "rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# grab rgi names\n",
    "rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "print(len(rgis_to_analyze_df))\n",
    "# print(rgis_to_analyze[:10])\n",
    "# print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2286712b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 1 of 45: RGI60-01.10910  2.084 km2\n",
      "\n",
      "Starting 2 of 45: RGI60-01.00787  2.126 km2\n",
      "\n",
      "Starting 3 of 45: RGI60-01.23606  2.344 km2\n",
      "\n",
      "Starting 4 of 45: RGI60-01.15253  2.551 km2\n",
      "\n",
      "Starting 5 of 45: RGI60-01.03379  2.578 km2\n",
      "\n",
      "Starting 6 of 45: RGI60-01.16719  2.681 km2\n",
      "\n",
      "Starting 7 of 45: RGI60-01.17321  2.88 km2\n",
      "\n",
      "Starting 8 of 45: RGI60-01.13462  3.206 km2\n",
      "\n",
      "Starting 9 of 45: RGI60-01.13483  3.216 km2\n",
      "\n",
      "Starting 10 of 45: RGI60-01.02584  3.441 km2\n",
      "\n",
      "Starting 11 of 45: RGI60-01.03215  3.998 km2\n",
      "\n",
      "Starting 12 of 45: RGI60-01.01666  4.243 km2\n",
      "\n",
      "Starting 13 of 45: RGI60-01.12548  4.314 km2\n",
      "\n",
      "Starting 14 of 45: RGI60-01.13930  4.404 km2\n",
      "\n",
      "Starting 15 of 45: RGI60-01.09624  4.487 km2\n",
      "\n",
      "Starting 16 of 45: RGI60-01.15516  4.764 km2\n",
      "\n",
      "Starting 17 of 45: RGI60-01.21721  6.422 km2\n",
      "\n",
      "Starting 18 of 45: RGI60-01.10255  7.262 km2\n",
      "\n",
      "Starting 19 of 45: RGI60-01.12165  7.969 km2\n",
      "\n",
      "Starting 20 of 45: RGI60-01.05007  9.216 km2\n",
      "\n",
      "Starting 21 of 45: RGI60-01.01104  9.528 km2\n",
      "\n",
      "Starting 22 of 45: RGI60-01.12186  11.05 km2\n",
      "\n",
      "Starting 23 of 45: RGI60-01.09656  13.791 km2\n",
      "\n",
      "Starting 24 of 45: RGI60-01.17784  14.773 km2\n",
      "\n",
      "Starting 25 of 45: RGI60-01.14493  15.336 km2\n",
      "\n",
      "Starting 26 of 45: RGI60-01.23643  15.732 km2\n",
      "\n",
      "Starting 27 of 45: RGI60-01.01270  16.163 km2\n",
      "\n",
      "Starting 28 of 45: RGI60-01.09162  16.749 km2\n",
      "\n",
      "Starting 29 of 45: RGI60-01.05078  17.259 km2\n",
      "\n",
      "Starting 30 of 45: RGI60-01.00570  17.567 km2\n",
      "\n",
      "Starting 31 of 45: RGI60-01.00557  18.042 km2\n",
      "\n",
      "Starting 32 of 45: RGI60-01.09216  18.634 km2\n",
      "\n",
      "Starting 33 of 45: RGI60-01.26731  20.207 km2\n",
      "\n",
      "Starting 34 of 45: RGI60-01.00565  23.06 km2\n",
      "\n",
      "Starting 35 of 45: RGI60-01.08989  29.395 km2\n",
      "\n",
      "Starting 36 of 45: RGI60-01.16166  29.932 km2\n",
      "\n",
      "Starting 37 of 45: RGI60-01.15731  40.009 km2\n",
      "\n",
      "Starting 38 of 45: RGI60-01.09798  41.785 km2\n",
      "\n",
      "Starting 39 of 45: RGI60-01.01743  45.165 km2\n",
      "\n",
      "Starting 40 of 45: RGI60-01.15135  66.067 km2\n",
      "\n",
      "Starting 41 of 45: RGI60-01.19542  71.722 km2\n",
      "\n",
      "Starting 42 of 45: RGI60-01.20841  80.284 km2\n",
      "\n",
      "Starting 43 of 45: RGI60-01.03741  136.264 km2\n",
      "\n",
      "Starting 44 of 45: RGI60-01.16558  343.098 km2\n",
      "\n",
      "Starting 45 of 45: RGI60-01.01390  521.396 km2\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "#     if i!=1: continue\n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "\n",
    "    # check if we've already run this glacier. if so, skip\n",
    "    temp_path = os.path.join(folder_save, 'Annual AAs', 'csv', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "#     if os.path.exists(temp_path):\n",
    "#         continue\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "\n",
    "#     if ga<300: continue\n",
    "    \n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\")\n",
    "    \n",
    "    # open the list of the % observed on each date, add a column which will hold post-smoothing %\n",
    "    obs_df_path = os.path.join(folder_save, 'Daily AAs', 'observed', f\"S2_{rgiid}_observed.csv\")\n",
    "    obs_df = pd.read_csv(obs_df_path).rename({'Date':'date'}, axis=1)\n",
    "    \n",
    "    # open the snow fraction dataframes\n",
    "    df_snow = pd.read_csv(os.path.join(folder_sca, f\"S2_{rgiid}_snow.csv\"))\n",
    "    df_observed = pd.read_csv(os.path.join(folder_sca, f\"S2_{rgiid}_observed.csv\"))\n",
    "\n",
    "    # define the columns that we will be manipulating (ie not the z_min, z_max, etc...)\n",
    "#     meta_columns = [\"z_min\", \"z_max\", \"total_pixels\"]\n",
    "    meta_columns = ['z_min', 'z_max'] + [ f\"total_pixels_{y}\" for y in [2018,2019,2020,2021,2022] ]\n",
    "    data_cols = df_snow.columns.difference(meta_columns)\n",
    "    \n",
    "    ## we need to remove elevation bands that have essentially no observations.\n",
    "    # for each band, calculate average of [ f\"total_pixels_{y}\" for y in [2018,2019,2020,2021,2022] ]\n",
    "    # if average is less than 50, remove it\n",
    "    average_pixel_count = df_observed[ [ f\"total_pixels_{y}\" for y in [2018,2019,2020,2021,2022] ] ].mean(axis=1)\n",
    "    good_bands = (average_pixel_count>50)\n",
    "    \n",
    "    df_snow = df_snow[good_bands].reset_index()\n",
    "    df_observed = df_observed[good_bands].reset_index()\n",
    "    \n",
    "    # get high, low, mid elevation\n",
    "    zmin = np.nanmin(df_snow['z_min'])\n",
    "    zmax = np.nanmin(df_snow['z_max'])\n",
    "    zmid = np.nanmedian(df_snow['z_min'])\n",
    "    upper_eles = (df_snow['z_min']>zmid)\n",
    "    lower_eles = (df_snow['z_min']<=zmid)\n",
    "    \n",
    "    # create df with how much of each band was NOT observed\n",
    "    df_not_observed = df_observed.copy()\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        data_cols_y = [ d for d in data_cols if int(d[:4])==y ]\n",
    "        for d in data_cols_y:\n",
    "            to_fill = df_observed[ f\"total_pixels_{y}\" ] - df_not_observed[d]\n",
    "            to_fill[lower_eles] = 0\n",
    "            df_not_observed[d] = to_fill\n",
    "    \n",
    "    # calculate the fraction of the entire glacier surface that is visible on each date (after smoothing)\n",
    "    df_observed_frac_glacier = df_observed[data_cols].sum(axis=0) / df_observed['total_pixels_2018'].sum()\n",
    "    \n",
    "    # optionally, make an assumption that everything that wasn't observed in each band in upper elevations is snow\n",
    "    infill=0\n",
    "    if infill:\n",
    "        df_snow[data_cols] = df_snow[data_cols] + df_not_observed[data_cols]\n",
    "        df_observed[data_cols] = df_observed[data_cols] + df_not_observed[data_cols]\n",
    "    \n",
    "    # calculate fraction of each band that was observed on each date\n",
    "    # have to do this one year at a time, because the total area in each band changes each year\n",
    "    df_observed_frac = df_observed[data_cols].copy()\n",
    "    df_snow_frac = df_snow[data_cols].copy()\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        \n",
    "        # grab column names from this year\n",
    "        data_cols_y = [ d for d in data_cols if int(d[:4])==y ]\n",
    "        \n",
    "        # grab the area of each band this year\n",
    "        total_pixels_y = df_snow[ f\"total_pixels_{y}\" ]\n",
    "        \n",
    "        # divide count of snow (and observed) in each band by the total pixels in each band that year\n",
    "        df_snow_frac[data_cols_y] = df_snow_frac[data_cols_y].divide(df_observed[data_cols_y]).fillna(0)\n",
    "        df_observed_frac[data_cols_y] = df_observed_frac[data_cols_y].divide(total_pixels_y, axis=0).fillna(0)\n",
    "                          \n",
    "    # calculate the glacier-wide aar on each date (based on observable surface)\n",
    "    df_aars = df_snow[data_cols].sum(axis=0) / df_observed[data_cols].sum(axis=0)                  \n",
    "    \n",
    "    # transition to numpy arrays for a bit. snow fractions >=0.5 become 1 (accumulation zone)\n",
    "    # everything <0.5 becomes 0 (ablation)\n",
    "    np_accumulation = df_snow_frac.copy()\n",
    "    np_accumulation[np_accumulation>=0.5] = 1\n",
    "    np_accumulation[np_accumulation<0.5] = 0\n",
    "    \n",
    "    # Define a kernel that sums the next 5 (4,3,2,1...) values along the 2nd dimension\n",
    "    kernel5 = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    kernel4 = np.array([1, 1, 1, 1, 0, 0, 0])\n",
    "    kernel3 = np.array([1, 1, 1, 0, 0])\n",
    "    \n",
    "    # apply these kernels, find elevation bands that are the start of 5 accumulation bands in a row (or 4,3,2,1)\n",
    "    all_elas_5 = (np.apply_along_axis(lambda x: np.convolve(x, kernel5, mode='same'), axis=0, arr=np_accumulation))==5\n",
    "    all_elas_4 = (np.apply_along_axis(lambda x: np.convolve(x, kernel4, mode='same'), axis=0, arr=np_accumulation))==4\n",
    "    all_elas_3 = (np.apply_along_axis(lambda x: np.convolve(x, kernel3, mode='same'), axis=0, arr=np_accumulation))==3\n",
    "\n",
    "    # so the lowest elevation point in each time that is equal to 5 (4,3,2,1) will be the ela\n",
    "    first_elas_5 = np.argmax(all_elas_5, axis=0).astype(float)\n",
    "    first_elas_4 = np.argmax(all_elas_4, axis=0).astype(float)\n",
    "    first_elas_3 = np.argmax(all_elas_3, axis=0).astype(float)\n",
    "    \n",
    "    # make sure that if a column has all 0s then we put nan, to allow filling in\n",
    "    first_elas_5[(all_elas_5.sum(axis=0)==0)] = np.nan\n",
    "    first_elas_4[(all_elas_4.sum(axis=0)==0)] = np.nan\n",
    "    first_elas_3[(all_elas_3.sum(axis=0)==0)] = np.nan\n",
    "    \n",
    "    # get the final elas, by first taking from 5, then 4, then 3\n",
    "    final_elas = first_elas_5\n",
    "    final_elas[np.isnan(final_elas)] = first_elas_4[np.isnan(final_elas)]\n",
    "    final_elas[np.isnan(final_elas)] = first_elas_3[np.isnan(final_elas)]\n",
    "\n",
    "    # lastly, if we still have no ela (the entire glacier is ablation) we'll put the highest elevation band as the ela\n",
    "    # make a separate flag for these\n",
    "    off_glacier_flag = np.isnan(final_elas)\n",
    "    final_elas[np.isnan(final_elas)] = np_accumulation.shape[0]-1\n",
    "    \n",
    "    # create series to link ela band # to band elevation\n",
    "    bands_zs = (df_snow['z_min'])\n",
    "    \n",
    "    # now that we have our elas for each time step, lets format into a df with date, ela, aar, and save\n",
    "    ela_df = pd.DataFrame({'date':data_cols})\n",
    "    ela_df['ela'] = [bands_zs[int(i)] for i in final_elas]\n",
    "    ela_df['fraction_observed_slopemasked'] = df_observed_frac_glacier.values.round(4)\n",
    "    ela_df['aar'] = df_aars.values.round(4)\n",
    "    ela_df['off_glacier'] = off_glacier_flag\n",
    "    \n",
    "    # add in the initial observed percent and % terrain shadow info\n",
    "    ela_df = ela_df.merge(obs_df, how='left', on='date')\n",
    "    \n",
    "    out_path = os.path.join(folder_save, \"ELAs\", f\"{rgiid}_ELAs.csv\")\n",
    "    ela_df.to_csv(out_path, index=False)\n",
    "    \n",
    "print('All done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
