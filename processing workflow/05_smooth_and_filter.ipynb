{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56b73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 1\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# open list of validation glaciers\n",
    "all_validation_df = pd.read_csv(os.path.join(folder_AGVA, 'Validation', 'Validation Glaciers.csv'))\n",
    "\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# select which rgis to analyze\n",
    "if validation_only:\n",
    "    rgis_to_analyze = list( set(rgis_folder).intersection(set(all_validation_df['RGIId'].values)) )\n",
    "else:\n",
    "    # rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "    rgis_to_analyze = rgis_folder # everything that is available\n",
    "#     rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# get list of glacier area for each rgi\n",
    "areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# make df\n",
    "rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "\n",
    "# sort however you want\n",
    "rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# grab rgi names\n",
    "rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "\n",
    "print(len(rgis_to_analyze_df))\n",
    "# print(rgis_to_analyze[:10])\n",
    "# print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d78c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 1 of 45: RGI60-01.10910  2.084 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 2 of 45: RGI60-01.00787  2.126 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 3 of 45: RGI60-01.23606  2.344 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 4 of 45: RGI60-01.15253  2.551 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 5 of 45: RGI60-01.03379  2.578 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 6 of 45: RGI60-01.16719  2.681 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 7 of 45: RGI60-01.17321  2.88 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 8 of 45: RGI60-01.13462  3.206 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 9 of 45: RGI60-01.13483  3.216 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 10 of 45: RGI60-01.02584  3.441 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 11 of 45: RGI60-01.03215  3.998 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 12 of 45: RGI60-01.01666  4.243 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 13 of 45: RGI60-01.12548  4.314 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 14 of 45: RGI60-01.13930  4.404 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 15 of 45: RGI60-01.09624  4.487 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 16 of 45: RGI60-01.15516  4.764 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 17 of 45: RGI60-01.21721  6.422 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 18 of 45: RGI60-01.10255  7.262 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 19 of 45: RGI60-01.12165  7.969 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 20 of 45: RGI60-01.05007  9.216 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 21 of 45: RGI60-01.01104  9.528 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 22 of 45: RGI60-01.12186  11.05 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 23 of 45: RGI60-01.09656  13.791 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 24 of 45: RGI60-01.17784  14.773 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 25 of 45: RGI60-01.14493  15.336 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 26 of 45: RGI60-01.23643  15.732 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 27 of 45: RGI60-01.01270  16.163 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 28 of 45: RGI60-01.09162  16.749 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 29 of 45: RGI60-01.05078  17.259 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 30 of 45: RGI60-01.00570  17.567 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 31 of 45: RGI60-01.00557  18.042 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 32 of 45: RGI60-01.09216  18.634 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 33 of 45: RGI60-01.26731  20.207 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 34 of 45: RGI60-01.00565  23.06 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 35 of 45: RGI60-01.08989  29.395 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 36 of 45: RGI60-01.16166  29.932 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 37 of 45: RGI60-01.15731  40.009 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 38 of 45: RGI60-01.09798  41.785 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 39 of 45: RGI60-01.01743  45.165 km2\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 40 of 45: RGI60-01.15135  66.067 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 41 of 45: RGI60-01.19542  71.722 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 42 of 45: RGI60-01.20841  80.284 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Smoothed daily AAs made\n",
      "\n",
      "Starting 43 of 45: RGI60-01.03741  136.264 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Smoothed daily AAs made\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "#     if i>0: continue\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "#     if rgiid!=\"RGI60-01.21721\": continue\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "    \n",
    "    if ga>300: continue\n",
    "    \n",
    "    # set folder\n",
    "    if validation_only:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation')\n",
    "    else:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "       \n",
    "    # check if this glacier has been run already, skip if so\n",
    "    if ga>500:\n",
    "        temp_path = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_2022_daily_AAs_shadowed_coarse_smoothed.nc\")\n",
    "    else:\n",
    "        temp_path = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_2022_daily_AAs_shadowed_smoothed.nc\")\n",
    "#     if os.path.exists(temp_path):\n",
    "#         continue\n",
    "    \n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\")\n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    " \n",
    "    # define the coarsen scale\n",
    "    if ga>1000:\n",
    "        scale=5\n",
    "    elif ga>500:\n",
    "        scale=3\n",
    "        \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto').glacier\n",
    "    \n",
    "    if ga>500:\n",
    "        glacier_mask = glacier_mask.coarsen({\"x\":scale, \"y\":scale}, boundary=\"trim\").median(skipna=True).astype('uint8')\n",
    "    \n",
    "    glacier_pixels = glacier_mask.sum().values\n",
    "\n",
    "    # open the list of the % observed on each date, add a column which will hold post-smoothing %\n",
    "    obs_df_path = os.path.join(folder_save, 'Daily AAs', 'observed', f\"S2_{rgiid}_observed.csv\")\n",
    "    obs_df = pd.read_csv(obs_df_path)\n",
    "    obs_df['observed_after_smoothing'] = np.zeros(len(obs_df))\n",
    "    \n",
    "    # get the dates that have >5% observed\n",
    "    usable_dates = obs_df[obs_df['observed_initial']>0.05]['Date']#.values\n",
    "    \n",
    "    # doing rolling smoothing one year at a time\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "#         if y!=2018: continue\n",
    "        if ga>50: print(y)\n",
    "        \n",
    "        # open the file. chunking and getting coarsened products depending on size\n",
    "        if ga>500:\n",
    "            path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed_coarse.nc\")\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':1})\n",
    "        \n",
    "        elif ga>150:\n",
    "            path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed.nc\")\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':10})\n",
    "        \n",
    "        else: \n",
    "            path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed.nc\")\n",
    "            snow = xr.open_dataset(path_open) # if small glacier, we dont need to chunk\n",
    "        \n",
    "        # copy of snow oring\n",
    "        copy_s = snow.copy()\n",
    "        \n",
    "        # subset usable dates to this year\n",
    "        usable_dates_y = [ i for i in usable_dates.values if i[:4]==str(y)]\n",
    "\n",
    "        # if 'coarse' comes in as 0(nodata), 1(ablation), 2(snow)\n",
    "        # if not coarse, comes in as 0(nodata), 1(snow), 2-5(ablation)\n",
    "        # we want to make 1=snow, 0=ablation, nan=cloud,shadow,off-glacier\n",
    "        if ga>500:\n",
    "            snow = xr.where(snow==0, np.nan, xr.where(snow==1, 0, 1))\n",
    "        else:\n",
    "            snow = snow.where(snow!=0, np.nan).where(snow<=1, 0)\n",
    "\n",
    "        # subset to the \"good\" dates (remove those with no usable observations)\n",
    "        snow = snow.sel(time=usable_dates_y)\n",
    "        copy_s = copy_s.sel(time=usable_dates_y)\n",
    "        \n",
    "        # get list of the date of every observation\n",
    "        time_values = pd.to_datetime(snow.time.values)\n",
    "#         print(time_values.values)\n",
    "        \n",
    "        # now iterate through each date. get all obs from up to 15 days before, average through time\n",
    "        all_smoothed = []\n",
    "        for i in range(len(time_values)):\n",
    "#             if i>0: continue\n",
    "                \n",
    "            # grab this date, define the range of dates that will be used for infilling\n",
    "            date = time_values[i]\n",
    "            diffs = time_values-date\n",
    "            days_plus = 3\n",
    "            days_minus = 7\n",
    "        \n",
    "            # subset to the wanted dates (plus or minus 5 days)\n",
    "            good_dates = time_values[(diffs>=f\"-{days_minus}d\") & (diffs<=f\"{days_plus}d\")].values #time_values[(diffs>=\"-15d\") & (diffs<=\"0d\")]\n",
    "#             print(date, good_dates)\n",
    "            \n",
    "            # select obs from these dates, take average\n",
    "            if len(good_dates)==1: # we have to treat it differently if there is only 1 \"good date\"\n",
    "                smoothed = snow.sel(time = slice(good_dates[0], good_dates[-1]))['class']\n",
    "#                 smoothed = snow.sel(time=good_dates)\n",
    "            else:\n",
    "                smoothed = snow.sel(time = slice(good_dates[0], good_dates[-1])).mean(dim='time', skipna=True).expand_dims(time=[date])['class']\n",
    "#                 smoothed = snow.sel(time=good_dates).mean(dim='time', skipna=True)['class']\n",
    "            \n",
    "            # fix to 0(nodata), 1(ablation), 2(snow)\n",
    "            smoothed = xr.where(smoothed.isnull(), 0, xr.where(smoothed>=0.5, 2, 1)).astype('uint8')#.expand_dims(time=[date])\n",
    "#             print(smoothed)\n",
    "            \n",
    "            # count what fraction of glacier is observed now, add to df\n",
    "            observed_today = (xr.where(smoothed>0, 1, 0).sum(dim=['x','y'])/glacier_pixels).values\n",
    "#             print(observed_today)\n",
    "#             print(observed_today.round(4))\n",
    "            \n",
    "#             print(obs_df.loc[obs_df['Date'] == str(date)[:10]])\n",
    "            obs_df.loc[obs_df['Date'] == str(date)[:10], 'observed_after_smoothing'] = observed_today.round(4)\n",
    "#             print(obs_df.loc[obs_df['Date'] == str(date)[:10]])\n",
    "            \n",
    "            # save this to list\n",
    "            all_smoothed.append(smoothed)\n",
    "       \n",
    "        # now at the end concat them all together, sort by date\n",
    "        if len(all_smoothed)>0:\n",
    "            snow2 = xr.concat(all_smoothed, dim='time').sortby('time')\n",
    "\n",
    "            ### save\n",
    "            if ga>500:\n",
    "                path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed_coarse_smoothed.nc\")\n",
    "            else:\n",
    "                path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed_smoothed.nc\")\n",
    "            \n",
    "            save_xr = snow2.sel(time=slice(f\"{y}-01-01\", f\"{y}-12-31\")).astype('uint8').rename('class')\n",
    "\n",
    "            # specify compression/encoding\n",
    "            encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "            # save\n",
    "#             save_xr.to_netcdf(path_save, encoding=encoding)\n",
    "            \n",
    "#             del snow, all_smoothed, smoothed, snow2\n",
    "        \n",
    "        # save csv holding the % observed\n",
    "        obs_df.to_csv(obs_df_path, index=False)\n",
    "    \n",
    "    print(\"Smoothed daily AAs made\")\n",
    "    \n",
    "print(\"Done!\")\n",
    "#     if ga>500:\n",
    "#         for y in (2018,2019,2020,2021,2022):\n",
    "#             os.remove(os.path.join(folder_save, f'temp{y}.nc'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable = xr.where(snow2==0,0,1).sum(dim='time')#.to_array()\n",
    "\n",
    "# v1 = xr.where(snow2==2,1,0).sum(dim='time')/usable#.to_array()\n",
    "# v2 = xr.where(snow2==1,1,0).sum(dim='time')/usable#.to_array()\n",
    "\n",
    "# v = ((v1+1)/(v2+1))/glacier_mask\n",
    "# v3 = xr.where(v>1,1,0)/glacier_mask\n",
    "\n",
    "# fig,axs = plt.subplots(1,4, figsize=(15,5))\n",
    "# v1.plot(ax=axs[0])\n",
    "# v2.plot(ax=axs[1])\n",
    "# usable.plot(ax=axs[2])\n",
    "# v.plot(ax=axs[3])\n",
    "\n",
    "# for ax in axs:\n",
    "#     ax.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
