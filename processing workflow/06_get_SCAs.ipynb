{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe930ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "import importlib\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "folder_slope = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Slopes')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 1\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# open list of validation glaciers\n",
    "all_validation_df = pd.read_csv(os.path.join(folder_AGVA, 'Validation', 'Validation Glaciers.csv'))\n",
    "\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# select which rgis to analyze\n",
    "if validation_only:\n",
    "    rgis_to_analyze = list( set(rgis_folder).intersection(set(all_validation_df['RGIId'].values)) )\n",
    "else:\n",
    "    # rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "    rgis_to_analyze = rgis_folder # everything that is available\n",
    "#     rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# get list of glacier area for each rgi\n",
    "areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# make df\n",
    "rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "\n",
    "# sort however you want\n",
    "rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# grab rgi names\n",
    "rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "\n",
    "print(len(rgis_to_analyze_df))\n",
    "# print(rgis_to_analyze[:10])\n",
    "# print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf1bb46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 44 of 45: RGI60-01.16558  343.098 km2\n",
      "starting SCAs 2018\n",
      "starting SCAs 2019\n",
      "starting SCAs 2020\n",
      "starting SCAs 2021\n",
      "starting SCAs 2022\n",
      "\n",
      "Starting 45 of 45: RGI60-01.01390  521.396 km2\n",
      "starting SCAs 2018\n",
      "starting SCAs 2019\n",
      "starting SCAs 2020\n",
      "starting SCAs 2021\n",
      "starting SCAs 2022\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "importlib.reload(snowFun)\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "#     if i>0: continue\n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "    \n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "    \n",
    "    if ga<300: continue\n",
    "    \n",
    "    # set folder\n",
    "    if validation_only:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation')\n",
    "    else:\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "       \n",
    "    # check if this glacier has been run already, skip if so\n",
    "#     temp_path = os.path.join(folder_save, 'Band SCFs', f\"S2_{rgiid}_observed.csv\")\n",
    "#     if os.path.exists(temp_path):\n",
    "#         continue\n",
    "    \n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\") \n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    "    \n",
    "    # define the coarsen scale\n",
    "    if ga>1000:\n",
    "        scale=5\n",
    "    elif ga>500:\n",
    "        scale=3\n",
    "        \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto').glacier\n",
    "    \n",
    "    # open glacier slope\n",
    "    glacier_slope = xr.open_dataset(os.path.join(folder_slope, f\"S2_{rgiid}_slope.nc\"), chunks='auto').slope\n",
    "    \n",
    "    # define max slope that is okay\n",
    "    max_slope = 25    \n",
    "    \n",
    "    if ga>500:\n",
    "        glacier_mask = glacier_mask.coarsen({\"x\":scale, \"y\":scale}, boundary=\"trim\").median(skipna=True).astype('uint8')  \n",
    "    \n",
    "    ### lastly calculate elas for each year, pick the best, save all our products\n",
    "    csv_data = []\n",
    "\n",
    "    # we'll store everything in a single csv (all 5 years)\n",
    "    snow_dfs = []\n",
    "    obs_dfs = []\n",
    "    meta_dfs = []\n",
    "    \n",
    "    # open the list of the % observed on each date\n",
    "    obs_df_path = os.path.join(folder_save, 'Daily AAs', 'observed', f\"S2_{rgiid}_observed.csv\")\n",
    "    obs_df = pd.read_csv(obs_df_path)\n",
    "    \n",
    "    # get the dates that have >30% initial observed. we'll drop all others and not send them to have scfs calculated\n",
    "    usable_dates = obs_df[obs_df['observed_initial']>0.3]['Date']#.values\n",
    "    \n",
    "    # function that will re-order the columns in dfs before saving\n",
    "    def sort_cols(df):\n",
    "        first_cols = [\"total_pixels_2018\", \"total_pixels_2019\", \"total_pixels_2020\", \"total_pixels_2021\", \"total_pixels_2022\"]\n",
    "        other_cols = [ c for c in df.columns.tolist() if c not in first_cols]\n",
    "        new_order = first_cols + other_cols\n",
    "        return df[new_order]\n",
    "    \n",
    "    for y in [2018,2019,2020,2021,2022]: #2018,2019,2020,2021,2022\n",
    "#         print(y)\n",
    "        \n",
    "        print(f'starting SCAs {y}')\n",
    "        \n",
    "        if ga>500:\n",
    "            path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed_coarse_smoothed.nc\")\n",
    "        else:\n",
    "            path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed_smoothed.nc\")\n",
    "        \n",
    "        # if this path doesn't exist, add it to list of known problem glaciers/years\n",
    "        if not os.path.exists(path_open):\n",
    "            print('uhoh')\n",
    "            problem_file = os.path.join(folder_AGVA, 'Derived products', 'S2', 'problems.txt')\n",
    "            to_write = f\"{rgiid}_{y}\\n\"\n",
    "            \n",
    "            if to_write not in open(problem_file).read():\n",
    "                with open(problem_file, 'a') as file:\n",
    "                    file.write(to_write)               \n",
    "                \n",
    "            continue\n",
    "        \n",
    "        if ga>500:\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':1, 'y':-1, 'x':-1})['class']\n",
    "        elif ga>150:\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':10, 'y':-1, 'x':-1})['class'] # nodata=0, ablation=1, snow=2\n",
    "        else:\n",
    "            snow = xr.open_dataset(path_open)['class']\n",
    "    \n",
    "        # make snow to nan, 0, 1 (instead of 0,1,2) \n",
    "        snow = xr.where(snow==0, np.nan, snow-1)\n",
    "\n",
    "        # open dem\n",
    "        xr_dem = snowFun.get_year_DEM(single_geometry, y, smoothed=0) \n",
    "        \n",
    "        # if problems with dem, skip\n",
    "        if len(xr_dem.x)==0 or len(xr_dem.y)==0:\n",
    "            problem_file = os.path.join(folder_AGVA, 'Derived products', 'S2', 'problems.txt')\n",
    "            to_write = f\"{rgiid} dem\\n\"\n",
    "            \n",
    "            if to_write not in open(problem_file).read():\n",
    "                with open(problem_file, 'a') as file:\n",
    "                    file.write(to_write)               \n",
    "                \n",
    "            continue\n",
    "        \n",
    "        # apply max_slope to dem. anything greater than max_slope goes to 0\n",
    "        xr_dem = xr_dem.where(glacier_slope<=max_slope, 0)\n",
    "\n",
    "        # coarsen dem\n",
    "        if ga>500:\n",
    "            xr_dem = xr_dem.sel({\"x\":snow.x, \"y\":snow.y})\n",
    "\n",
    "        # shave off edges to make sure dem, mask match\n",
    "        glacier_mask = glacier_mask.reindex\n",
    "        xr_dem = xr_dem.reindex_like(glacier_mask, method='nearest')\n",
    "#         xr_dem = xr_dem.sel(x=slice( min(glacier_mask.x.values), max(glacier_mask.x.values) ), y=slice(max(glacier_mask.y.values),min(glacier_mask.y.values)))\n",
    "        glacier_mask = glacier_mask.sel(x=slice( min(xr_dem.x.values), max(xr_dem.x.values) ), y=slice(max(xr_dem.y.values),min(xr_dem.y.values)))\n",
    "\n",
    "        # resample (coarsen) dem and mask if very large glacier\n",
    "#         if ga>500:\n",
    "#             xr_dem = xr_dem.coarsen({\"x\":5, \"y\":5}, boundary=\"trim\").mean(skipna=True)\n",
    "#             glacier_mask = glacier_mask.coarsen({\"x\":5, \"y\":5}, boundary=\"trim\").median(skipna=True).astype(\"uint8\")\n",
    "            \n",
    "        # shave off edges to make sure dem, mask, snow match\n",
    "        snow = snow.sel(x=slice( min(xr_dem.x.values), max(xr_dem.x.values) ), y=slice(max(xr_dem.y.values),min(xr_dem.y.values)))\n",
    "        \n",
    "#         print(snow.shape)\n",
    "#         print(xr_dem.shape)\n",
    "#         print(glacier_mask.shape)\n",
    "\n",
    "        # subset to only the usable dates this year\n",
    "        usable_dates_y = [ i for i in usable_dates.values if i[:4]==str(y)]\n",
    "        snow = snow.sel(time=usable_dates_y)\n",
    "        \n",
    "        # extract number of snow pixels in bands from each time step\n",
    "        scas = snowFun.extract_band_SCA(snow, xr_dem, glacier_mask, step=10)\n",
    "        scas = scas.rename(columns={'total_pixels':f\"total_pixels_{y}\"})\n",
    "        snow_dfs.append(scas)\n",
    "        \n",
    "        # extract number of pixels observed in bands from each time step\n",
    "        scas = snowFun.extract_band_SCA( xr.where(snow>=0, 1, 0) , xr_dem, glacier_mask, step=10)\n",
    "        scas = scas.rename(columns={'total_pixels':f\"total_pixels_{y}\"})\n",
    "        obs_dfs.append(scas)\n",
    "\n",
    "    if len(snow_dfs)>0:\n",
    "        # concat all the years together, save to csv for SNOW\n",
    "        result_df = pd.concat([df.set_index(['z_min','z_max']) for df in snow_dfs], axis=1, join='outer', sort=True).fillna(0)\n",
    "        result_df = sort_cols(result_df)\n",
    "\n",
    "        # save to csv\n",
    "        path_save = os.path.join(folder_save, 'Band SCFs', f\"S2_{rgiid}_snow.csv\")\n",
    "        result_df.to_csv(path_save, index=True)\n",
    "\n",
    "        # concat all the years together, save to csv for OBSERVED\n",
    "        result_df = pd.concat([df.set_index(['z_min','z_max']) for df in obs_dfs], axis=1, join='outer', sort=True).fillna(0)\n",
    "        result_df = sort_cols(result_df)\n",
    "\n",
    "        # save to csv\n",
    "        path_save = os.path.join(folder_save, 'Band SCFs', f\"S2_{rgiid}_observed.csv\")\n",
    "        result_df.to_csv(path_save, index=True)\n",
    "    \n",
    "    else:\n",
    "        print(f\"No data for {rgiid}\")\n",
    "        \n",
    "print(\"Done!\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
