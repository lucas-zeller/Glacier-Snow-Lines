{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed27064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi, drop='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f735687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 0\n",
    "\n",
    "# select which rgis to analyze\n",
    "if validation_only:\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Validation')\n",
    "    \n",
    "else:\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "\n",
    "# get list of rgis to analyze\n",
    "# rgis_to_analyze = list(set( [ i[3:17] for i in os.listdir(os.path.join(folder_save, \"Average AAs\")) if i[-3:]=='tif' ] ))\n",
    "\n",
    "# # get list of glacier area for each rgi\n",
    "# areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# # make df\n",
    "# rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "\n",
    "# # sort however you want\n",
    "# rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# # grab rgi names\n",
    "# rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "# print(len(rgis_to_analyze_df))\n",
    "# # print(rgis_to_analyze[:10])\n",
    "# # print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd66203",
   "metadata": {},
   "source": [
    "### First step: open glacier DEM and calculate hypsometric ELA given observed AAR, hypsometric AAR given ELA\n",
    "#### Then calculate the AAR if we raised ELA 10m, 20m, 30m, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08981b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open final aar/ela df\n",
    "df_path = os.path.join(folder_save, 'Average AAs', 'final_glacier_stats.csv')\n",
    "df_obs = pd.read_csv(df_path)\n",
    "\n",
    "# get rgi list\n",
    "rgis_to_analyze = df_obs['RGIId'].values\n",
    "\n",
    "# initiate list to hold outputs\n",
    "outputs = []\n",
    "\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "    \n",
    "#     if rgiid!=\"RGI60-01.09162\": continue\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "\n",
    "    # print progress\n",
    "    print(f\"Starting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\")\n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "    single_obs = df_obs[df_obs['RGIId']==rgiid]\n",
    "    \n",
    "    # define the coarsen scale\n",
    "#     if ga>3000:\n",
    "#         scale=5\n",
    "    if ga>1000:\n",
    "        scale=5\n",
    "    elif ga>500:\n",
    "        scale=3\n",
    "    else:\n",
    "        scale=1\n",
    "        \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto').glacier\n",
    "    \n",
    "    if ga>500:\n",
    "        glacier_mask = glacier_mask.coarsen({\"x\":scale, \"y\":scale}, boundary=\"trim\").median(skipna=True).astype('uint8')  \n",
    "\n",
    "    # open dem for year 2020\n",
    "    xr_dem = snowFun.get_year_DEM(single_geometry, 2020, smoothed=0) \n",
    "    \n",
    "    # coarsen dem\n",
    "    if ga>500:\n",
    "        xr_dem = xr_dem.sel({\"x\":glacier_mask.x, \"y\":glacier_mask.y})\n",
    "\n",
    "    # shave off edges to make sure dem, mask match\n",
    "    xr_dem = xr_dem.sel(x=slice( min(glacier_mask.x.values), max(glacier_mask.x.values) ), y=slice(max(glacier_mask.y.values),min(glacier_mask.y.values)))\n",
    "    glacier_mask = glacier_mask.sel(x=slice( min(xr_dem.x.values), max(xr_dem.x.values) ), y=slice(max(xr_dem.y.values),min(xr_dem.y.values)))\n",
    "\n",
    "    # flatten dem values into single list, removing 0s\n",
    "    dem_values = np.ravel(xr_dem.values)\n",
    "    dem_values = dem_values[ (dem_values!=0) & ~np.isnan(dem_values)]\n",
    "    dem_length = len(dem_values)\n",
    "    \n",
    "    # get the observed aar, ela\n",
    "    obs_ela = single_obs['ela'].values[0]\n",
    "    obs_aar = single_obs['aar'].values[0]\n",
    "    \n",
    "    # define function that will calculate the expected AAR of a given ELA\n",
    "    def get_hyps_aar(ela_input):\n",
    "        return round( np.nansum( (dem_values>=ela_input) )/dem_length, 4)\n",
    "    \n",
    "    # calculate hypsometric aar, ela\n",
    "    hyps_ela = np.nanpercentile(dem_values, (1-obs_aar)*100)\n",
    "    hyps_aar = get_hyps_aar(obs_ela)\n",
    "    \n",
    "    # calculate hypsometric AAR if we raise ela by 100m\n",
    "    hyps_aar_20 = get_hyps_aar(obs_ela+20)\n",
    "    hyps_aar_40 = get_hyps_aar(obs_ela+40)\n",
    "    hyps_aar_60 = get_hyps_aar(obs_ela+60)\n",
    "    hyps_aar_80 = get_hyps_aar(obs_ela+80)\n",
    "    hyps_aar_100 = get_hyps_aar(obs_ela+100)\n",
    "    hyps_aar_120 = get_hyps_aar(obs_ela+120)\n",
    "    hyps_aar_140 = get_hyps_aar(obs_ela+140)\n",
    "    hyps_aar_160 = get_hyps_aar(obs_ela+160)\n",
    "    hyps_aar_180 = get_hyps_aar(obs_ela+180)\n",
    "    hyps_aar_200 = get_hyps_aar(obs_ela+200)\n",
    "    \n",
    "    # create dictionary to save\n",
    "    out_dic = {\"RGIId\":rgiid, \"obs_ela\":obs_ela, \"obs_aar\":obs_aar,\n",
    "               \"hyps_ela\":hyps_ela, \"hyps_aar\":hyps_aar,\n",
    "               \"hyps_aar_20\":hyps_aar_20, \"hyps_aar_40\":hyps_aar_40, \"hyps_aar_60\":hyps_aar_60, \"hyps_aar_80\":hyps_aar_80, \"hyps_aar_100\":hyps_aar_100,\n",
    "               \"hyps_aar_120\":hyps_aar_120, \"hyps_aar_140\":hyps_aar_140, \"hyps_aar_160\":hyps_aar_160, \"hyps_aar_180\":hyps_aar_180, \"hyps_aar_200\":hyps_aar_200}\n",
    "  \n",
    "    # save to list\n",
    "    outputs.append(out_dic)\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "# format to df, save\n",
    "out_df = pd.DataFrame(outputs)\n",
    "out_path = os.path.join(folder_save, 'Average AAs', 'hyspometric_stats.csv')\n",
    "out_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Given observed ELA of: {obs_ela}, we expect AAR of: {hyps_aar}\")\n",
    "print(f\"Given observed AAR of: {obs_aar}, we expect ELA of: {hyps_ela}\")\n",
    "print(f\"If ELA rises by 100 meters, the hypsometric AAR will be: {hyps_aar_100}\")\n",
    "print(f\"If ELA rises by 200 meters, the hypsometric AAR will be: {hyps_aar_200}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4819466",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a71605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d883ee",
   "metadata": {},
   "source": [
    "### Second step: go through those snow cover fraction products to calculate the ELA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
