{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebe930ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dask.array\n",
    "import importlib\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import snowFun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "folder_slope = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Slopes')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"rgi_2km_o3regions\", \"rgi_2km_o3regions.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "### choose if you want to do only the 45 validation glaciers\n",
    "validation_only = 0\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "folder_ela = os.path.join(folder_AGVA, 'Derived products', 'S2', 'ELAs')\n",
    "rgis_to_analyze = rgis_folder # everything that is available\n",
    "\n",
    "# get list of glacier area for each rgi\n",
    "areas = [rgi_gdf[rgi_gdf['RGIId']==i]['Area'].values for i in rgis_to_analyze]\n",
    "\n",
    "# make df\n",
    "rgis_to_analyze_df = pd.DataFrame({\"RGIId\":rgis_to_analyze, 'Area':areas})\n",
    "rgis_to_analyze_df = rgis_to_analyze_df[rgis_to_analyze_df['Area']>=500]\n",
    "\n",
    "# sort however you want\n",
    "rgis_to_analyze_df = rgis_to_analyze_df.sort_values('Area')\n",
    "\n",
    "# grab rgi names\n",
    "rgis_to_analyze = rgis_to_analyze_df['RGIId'].values\n",
    "\n",
    "\n",
    "print(len(rgis_to_analyze_df))\n",
    "# print(rgis_to_analyze[:10])\n",
    "# print(rgis_to_analyze_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87b2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = rgis_to_analyze_df.copy()\n",
    "out_df[['2018','2019','2020','2021','2022']] = 0\n",
    "out_path = os.path.join(folder_save, 'Big Glacier Check', \"first_look.csv\")\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    out_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf1bb46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 22 of 22: RGI60-01.13696  3362.656 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "importlib.reload(snowFun)\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "#     if i>0: continue\n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "#     if rgiid != \"RGI60-01.13696\": continue\n",
    "    \n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area'].values[0]\n",
    "    \n",
    "    if ga<500: continue\n",
    "        \n",
    "    c+=1\n",
    "#     if c>1: continue\n",
    "        \n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga} km2\") \n",
    "    \n",
    "    # create folder to save to if it doesn't exist\n",
    "    folder_outputs = os.path.join(folder_save, 'Big Glacier Check', rgiid)\n",
    "    if not os.path.exists(folder_outputs):\n",
    "        os.makedirs(folder_outputs)\n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    "    \n",
    "    # define the coarsen scale\n",
    "    if ga>1000:\n",
    "        scale=5\n",
    "    elif ga>500:\n",
    "        scale=3\n",
    "        \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto').glacier\n",
    "    if ga>500:\n",
    "        glacier_mask = glacier_mask.coarsen({\"x\":scale, \"y\":scale}, boundary=\"trim\").median(skipna=True).astype('uint8') \n",
    "#     print(glacier_mask)\n",
    "\n",
    "    # get max elevation\n",
    "    ele_max = snowFun.get_year_DEM(single_geometry, 2013, smoothed=0)\n",
    "    ele_max = ele_max.max()\n",
    "    \n",
    "    # open the full ela dataframe\n",
    "    path_open = os.path.join(folder_ela, f\"{rgiid}_ELAs.csv\")\n",
    "    ela_df = pd.read_csv(path_open)\n",
    "    \n",
    "    # throw out anything that has <XYZ% observed\n",
    "    ela_df_good = ela_df[ela_df['observed_after_smoothing']>0.7]\n",
    "    ela_df_good = ela_df_good[ela_df_good['observed_initial']>0.3]\n",
    "    \n",
    "    # open the originally IDed annual best data\n",
    "    path_open = os.path.join(folder_save, 'Annual AAs', 'csv', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "    ela_df_best = pd.read_csv(path_open)\n",
    "    \n",
    "    # add year, month, day of year to each df\n",
    "    ela_df_best['year'] = [ int(d[:4]) for d in ela_df_best['date'] ]\n",
    "    ela_df_best['month'] = [ int(d[5:7]) for d in ela_df_best['date'] ]\n",
    "    ela_df_best['doy'] = [ int(datetime.strptime(d, \"%Y-%m-%d\").timetuple().tm_yday) for d in ela_df_best['date'] ]\n",
    "    ela_df_good['year'] = [ int(d[:4]) for d in ela_df_good['date'] ]\n",
    "    ela_df_good['month'] = [ int(d[5:7]) for d in ela_df_good['date'] ]\n",
    "    ela_df_good['doy'] = [ int(datetime.strptime(d, \"%Y-%m-%d\").timetuple().tm_yday) for d in ela_df_good['date'] ]\n",
    "    ela_df_good['date_dt'] = pd.to_datetime(ela_df_good['date'])\n",
    "    \n",
    "#     for each year open the best image and save it\n",
    "    for y in [2018,2019,2020,2021,2022]: #2018,2019,2020,2021,2022\n",
    "        print(y)\n",
    "        \n",
    "        # subset each df to this year\n",
    "        best_subset = ela_df_best[ela_df_best['year']==y]\n",
    "        good_subset = ela_df_good[ela_df_good['year']==y]\n",
    "        \n",
    "        # get best date\n",
    "        best_date = best_subset['date'].values[0]\n",
    "        \n",
    "        # also subset to the 15 days before/after the 'best date'\n",
    "        doy = best_subset['doy'].values[0]\n",
    "        good_subset_subset = good_subset[(good_subset['doy']<=(doy+15)) & (good_subset['doy']>=(doy-15))]\n",
    "        \n",
    "        # option to override and get just a specific date range\n",
    "#         doy_range = [214,244]\n",
    "#         year_exact = 2019\n",
    "        \n",
    "        if doy_range:\n",
    "            if year_exact!=y: continue\n",
    "            good_subset_subset = good_subset[(good_subset['doy']<=doy_range[1]) & (good_subset['doy']>=doy_range[0])]\n",
    "\n",
    "        # open the final smoothed snow dataset\n",
    "        path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_shadowed_coarse_smoothed.nc\")\n",
    "        snow = xr.open_dataset(path_open, chunks={'time':1, 'y':-1, 'x':-1})['class'].squeeze()\n",
    "        \n",
    "        # now for each product in the good_subset_subset, make figure to save\n",
    "        for this_date in good_subset_subset['date'].values:\n",
    "        \n",
    "#             # grab the best data\n",
    "#             best_snow = snow.sel(time=best_date)\n",
    "\n",
    "            # grab the snow from this date\n",
    "            this_snow = snow.sel(time=this_date)\n",
    "\n",
    "            # fill in missing on-glacier data\n",
    "    #         print(glacier_mask)\n",
    "            glacier_mask = glacier_mask.reindex_like(this_snow, method='nearest')\n",
    "            this_snow = xr.where( (glacier_mask==0) & (this_snow==0), -1, this_snow )\n",
    "\n",
    "            ### make a figure to save\n",
    "            fig,axs=plt.subplots(2,1, figsize=(20,20), height_ratios=[4, 1])\n",
    "            axs[0].imshow(this_snow.values, vmin=-1, vmax=2, cmap='RdPu')\n",
    "            axs[0].contour(glacier_mask.values, levels=[0.5], colors='black')\n",
    "            axs[0].axis('off')\n",
    "            axs[0].set_title(this_date, size=7)\n",
    "\n",
    "            axs[1].scatter(good_subset['date_dt'],good_subset['ela'])\n",
    "            axs[1].vlines([this_date], ymin=0, ymax=ele_max)\n",
    "            axs[1].grid()\n",
    "            axs[1].set_ylim(0, ele_max)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if this_date == best_date:\n",
    "                name = f\"{rgiid}_{this_date}_BEST.png\"\n",
    "            else:\n",
    "                name = f\"{rgiid}_{this_date}.png\"\n",
    "            plt.savefig(os.path.join(folder_outputs, name))\n",
    "            plt.close()\n",
    "\n",
    "print(\"Done!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,1, figsize=(20,20), height_ratios=[4, 1])\n",
    "axs[0].imshow(best_snow.values, vmin=-1, vmax=2, cmap='RdPu')\n",
    "axs[0].contour(glacier_mask.values, levels=[0.5], colors='black')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title(best_date, size=7)\n",
    "\n",
    "axs[1].scatter(good_subset['date_dt'],good_subset['ela'])\n",
    "axs[1].vlines([best_date], ymin=0, ymax=np.nanmax(good_subset['ela']))\n",
    "\n",
    "axs[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "name = f\"{rgiid}_{best_date}.png\"\n",
    "# plt.savefig(os.path.join(folder_save, 'Big Glacier Check', name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bac056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xr.where( (glacier_mask==0) & (best_snow==0), -1, best_snow ))\n",
    "print()\n",
    "print(snow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
