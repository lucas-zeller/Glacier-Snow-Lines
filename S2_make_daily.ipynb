{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import snowFun\n",
    "import dask.array\n",
    "# %matplotlib widget\n",
    "\n",
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"01_rgi60_Alaska\", \"01_rgi60_Alaska.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate through glaciers, running the analysis for each\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# select which rgis to analyze\n",
    "# rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "# rgis_to_analyze = rgis_folder # everything that is available\n",
    "rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# sort\n",
    "rgis_to_analyze.sort()\n",
    "# print(rgis_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d78c89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 280 of 483: RGI60-01.10575  449.57 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Initial daily AAs made\n",
      "\n",
      "Starting 290 of 483: RGI60-01.10689  773.873 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Initial daily AAs made\n"
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "    \n",
    "    # options for skipping\n",
    "#     if (i+1)<296: continue #everything done to here\n",
    "#     if (i+1)>400: continue #everything done to here \n",
    "#     if (i+1) not in [405,406,407,408,420, 291]: continue\n",
    "    if (i+1) not in [280, 290]: continue\n",
    "#     if rgiid == \"RGI60-01.09162\": skip=0\n",
    "#     if skip: continue\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area']\n",
    "    \n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga.values[0]} km2\")\n",
    "#     break\n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    "    \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto') \n",
    "    \n",
    "    # open the classification data\n",
    "    file_name = f\"S2_{rgiid}_2018-01-01_2023-01-01\"\n",
    "    xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\")).chunk({'band':-1, 'y':1000, 'x':1000})#.rio.clip(single_geometry, from_disk=True, drop=True).chunk()\n",
    "\n",
    "    # load metadata csv, convert date to datetimes\n",
    "    meta_fp = os.path.join(folder_meta, f\"{file_name}.csv\")\n",
    "    meta_df = pd.read_csv(meta_fp)\n",
    "    \n",
    "    # format time axis to pandas datetime, like xarray wants\n",
    "    datetimes = pd.to_datetime([f\"{str(i)[:4]}-{str(i)[4:6]}-{str(i)[6:]}\" for i in meta_df['date']])\n",
    "    xr_class = xr_class.rename({\"band\":\"time\"})\n",
    "    xr_class['time'] = datetimes\n",
    "#     print(datetimes)\n",
    "\n",
    "    # merge images on same day, if there are repeated dates\n",
    "    if len(datetimes)!=len(datetimes.unique()):\n",
    "        xr_class = xr_class.where(xr_class<20, 0).groupby('time').max('time')\n",
    "    else:\n",
    "        xr_class = xr_class.where(xr_class<20, 0)\n",
    "\n",
    "    # get these merged dates\n",
    "    datetimes_merged = xr_class.time.values\n",
    "\n",
    "    ### create binary mask of useable and unuseable data, use it to mask the xr_class to only this area\n",
    "    bad_classes = [5] #class 5 is shadow. we will call these area unusable\n",
    "    good_classes = [1,2,3,4,6] #snow,firn,ice,debris,water are usable areas\n",
    "    # usable = xr.where( xr_class.isin(good_classes), 1, 0)\n",
    "\n",
    "    # count total number of pixels on the glacier surface, based on the glacier rgi area\n",
    "    glacier_pixels = int(ga * (1000*1000) / (10*10))\n",
    "#     glacier_pixels = glacier_mask.sum().values \n",
    "\n",
    "    # now we can mask out unusable areas in each time step\n",
    "    xr_class = xr.where(xr_class.isin(good_classes), xr_class, 0) #.sel(time=good_times) # note we subset to only the good days\n",
    "\n",
    "    # count useable pixels on each day\n",
    "#     count_usable_by_time = np.count_nonzero(xr_class, axis=(1,2))\n",
    "\n",
    "    # calculate percent of the glacier surface that is usable on each day\n",
    "#     percent_usable_by_time = count_usable_by_time/glacier_pixels\n",
    "    percent_usable_by_time = (xr.where(xr_class>0, 1, 0).sum(dim=['x','y'])/glacier_pixels)\n",
    "\n",
    "    # now lets throw out days where there is essentially no usable data\n",
    "    good_times = (percent_usable_by_time>0.05)\n",
    "\n",
    "    # print(xr_class)\n",
    "    xr_class = xr_class.sel(time=good_times)  # what if we don't calculate this here?\n",
    "    # print(xr_class.shape)\n",
    "\n",
    "    # at this point, xr_class is 0 off-glacier, 0 in shadow/cloud, and 1-6 in usable areas\n",
    "    # save indidivual years at this point\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "\n",
    "    # save one year at a time\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        print(y)\n",
    "        path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "\n",
    "        save_xr = xr_class.sel(time=slice(f\"{y-1}-12-01\", f\"{y+1}-01-31\")).astype('uint8').rename('class')\n",
    "\n",
    "        # specify compression/encoding\n",
    "        encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "        # save\n",
    "        save_xr.to_netcdf(path_save, encoding=encoding)\n",
    "    \n",
    "    print(\"Initial daily AAs made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4555f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4099d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
