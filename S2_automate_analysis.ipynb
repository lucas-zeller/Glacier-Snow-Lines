{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import snowFun\n",
    "# %matplotlib widget\n",
    "\n",
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"01_rgi60_Alaska\", \"01_rgi60_Alaska.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b044619",
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate through glaciers, running the analysis for each\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# set which rgis to analyze\n",
    "rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "rgis_to_analyze = rgis_folder # everything that is available\n",
    "rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "len(rgis_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e88b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(rgis_to_analyze)):\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "    \n",
    "    # print progress\n",
    "    print(f\"Starting {i+1} of {len(rgis_to_analyze)}: {rgiid}\")\n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    "\n",
    "    # open the glacier classification, clipping to the glacier outline\n",
    "    file_name = f\"S2_{rgiid}_2018-01-01_2023-01-01\"\n",
    "    xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\")).rio.clip(single_geometry, from_disk=True, drop=True)\n",
    "\n",
    "    # load metadata csv, convert date to datetimes\n",
    "    meta_fp = os.path.join(folder_meta, f\"{file_name}.csv\")\n",
    "    meta_df = pd.read_csv(meta_fp)\n",
    "\n",
    "    # format time axis to pandas datetime, like xarray wants\n",
    "    datetimes = pd.to_datetime([f\"{str(i)[:4]}-{str(i)[4:6]}-{str(i)[6:]}\" for i in meta_df['date']])\n",
    "    xr_class = xr_class.rename({\"band\":\"time\"})\n",
    "    xr_class['time'] = datetimes\n",
    "\n",
    "    # create quick binary glacier mask of 0 and 1\n",
    "    glacier_mask = xr_class.max(dim='time')\n",
    "    glacier_mask = (glacier_mask>0)#.astype(int)\n",
    "\n",
    "    # merge images on same day\n",
    "    xr_class = xr_class.where(xr_class<20, 0).groupby('time').max('time')\n",
    "\n",
    "    # get these merged dates\n",
    "    datetimes_merged = xr_class.time.values\n",
    "\n",
    "    # get base dem of the glacier\n",
    "    xr_dem = snowFun.get_base_DEM(single_geometry.values[0])\n",
    "\n",
    "    print(\"everything loaded\")\n",
    "\n",
    "    ### create binary mask of useable and unuseable data\n",
    "    bad_classes = [5] #class 5 is shadow. we will call these area unusable\n",
    "    good_classes = [1,2,3,4,6] #snow,firn,ice,debris,water are usable areas\n",
    "    usable = xr.where( xr_class.isin(good_classes), 1, 0)\n",
    "\n",
    "    # count total number of pixels on the glacier surface\n",
    "    glacier_pixels = glacier_mask.sum().values\n",
    "\n",
    "    # count usable pixels on each day\n",
    "    count_usable_by_time = usable.sum(dim=['x','y']) \n",
    "\n",
    "    # calculate percent of the glacier surface that is usable on each day\n",
    "    percent_usable_by_time = count_usable_by_time/glacier_pixels\n",
    "\n",
    "    # count number of total observations at each pixel (usable and not usable)\n",
    "    count_total_by_pixel = xr.where( xr_class>0, 1, 0).sum('time')\n",
    "\n",
    "    # count number of usable observations at each pixel\n",
    "    count_usable_by_pixel = usable.sum(dim='time') \n",
    "\n",
    "    # calculate the percentage of observations that are usable, at each pixel\n",
    "    percent_usable_by_pixel = count_usable_by_pixel/count_total_by_pixel # sum and divide by number of obs\n",
    "\n",
    "    # we will define the \"good days\", aka days where at least x% of the glacier surface was observed\n",
    "    # we will usually remove any other days before we apply any analysis\n",
    "    good_times = (percent_usable_by_time>0.1)\n",
    "\n",
    "    # now we can mask out unusable areas in each time step\n",
    "    # and then forward fill in the gaps using the most recent usable observation\n",
    "    xr_class_masked = xr.where(usable==1, xr_class, np.nan).sel(time=good_times) # note we subset to only the good days\n",
    "\n",
    "\n",
    "    # create raw snow product where snow=1, other class=0, cloud=np.nan\n",
    "    snow_masked = xr.where(xr_class_masked.isin([2,3,4,6]), 0, xr_class_masked)\n",
    "\n",
    "    # Create a new empty dataarray with the same x/y shape, but with 1-day frequency\n",
    "    time_values = pd.to_datetime(snow_masked.time.values)\n",
    "    new_time_values = pd.date_range(start=time_values.min(), end=time_values.max(), freq='D')\n",
    "    snow2 = xr.DataArray(0, coords={'time': new_time_values, 'y': snow_masked.y, 'x': snow_masked.x },\n",
    "                                  dims=('time', 'y', 'x'))\n",
    "\n",
    "    # Update the new DataArray with available data from the original DataArray\n",
    "    snow2 = snow_masked.broadcast_like(snow2)\n",
    "\n",
    "    # now use rolling window mean and then extract the original good obs\n",
    "    snow2 = (snow2.rolling(time=15, min_periods=1, center=False).mean(skipna=False)).loc[dict(time=time_values)]\n",
    "\n",
    "    ### make the floats to 0s and 1s, but preserve nans\n",
    "    snow2 = xr.where(snow2>=0.5, 1, snow2)\n",
    "    snow2 = xr.where(snow2<0.5, 0, snow2)\n",
    "    snow=snow2\n",
    "    \n",
    "    print(\"snow made\")\n",
    "\n",
    "    ### smooth a little bit using some convolution\n",
    "    snow_x = snow.rolling({'x':3}, min_periods=1, center=True).sum()\n",
    "    snow_x = snow_x.rolling({'y':3}, min_periods=1, center=True).sum()\n",
    "    norm_x = (snow>-1).rolling({'x':3}, min_periods=1, center=True).sum()\n",
    "    norm_x = norm_x.rolling({'y':3}, min_periods=1, center=True).sum()\n",
    "\n",
    "    snow_x = snow_x/norm_x # this show what fraction of the 3x3 box around each pixel is snow\n",
    "    snow_x = xr.where(snow_x>=0.5, 1, snow_x)\n",
    "    snow_x = xr.where(snow_x<0.5, 0, snow_x)\n",
    "    snow_x = xr.where(glacier_mask==1, snow_x, np.nan)\n",
    "\n",
    "    snow = snow_x\n",
    "\n",
    "    # we need to recount the pixels in each image, how much is observable, etc...\n",
    "    count_snow_by_time = snow.sum(dim=['x','y'], skipna=True) # total snow obs in each time\n",
    "    count_all_by_time = xr.where(snow>-1,1,0).sum(dim=['x','y'])\n",
    "    percent_all_by_time = count_all_by_time/glacier_pixels\n",
    "    percent_snow_by_time = count_snow_by_time/count_all_by_time\n",
    "\n",
    "    # filter out the dates that have less than x% usable data\n",
    "    usable_thresh = 0.85\n",
    "    snow = snow.sel(time=(percent_all_by_time>usable_thresh))\n",
    "    \n",
    "    print(\"snow filtered and smoothed\")\n",
    "\n",
    "    # extract ELAs from each time step\n",
    "    glacier_ELAs = snowFun.get_the_ELAs(snow, xr_dem, glacier_mask, step=20, width=1, p_snow=0.5)\n",
    "\n",
    "    # lets add aar on to the df as well\n",
    "    glacier_ELAs['aar'] = percent_snow_by_time.sel(time=(percent_all_by_time>usable_thresh))\n",
    "\n",
    "    # if ela is above the glacier then we get 9999. below and we get -1\n",
    "    # we can change these to the glacier min or max if we want (buffered by 1)\n",
    "    z_min = np.nanmin(xr_dem.where(xr_dem>0))\n",
    "    z_max = np.nanmax(xr_dem)\n",
    "    glacier_ELAs = glacier_ELAs.replace({'ela': {-1:z_min, 9999:z_max} })\n",
    "\n",
    "    ### calculate ideal elas for each aar\n",
    "    ideal_ELAs = snowFun.idealized_ELA_AAR(xr_dem, glacier_mask)\n",
    "\n",
    "    # lets use this aar-ela relationship to root out bad observations\n",
    "    # for each aar we observed, see what the ideal ela would be\n",
    "    glacier_ELAs['aar_round'] = glacier_ELAs['aar'].round(2)\n",
    "    glacier_ELAs['ela_ideal'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==i]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "\n",
    "    # how about we incorporate a little error and see the range of elas we could expect\n",
    "    error_allowed = 0.2\n",
    "    glacier_ELAs['ela_ideal_min'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==round(min(i+error_allowed,1),2)]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "    glacier_ELAs['ela_ideal_max'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==round(max(i-error_allowed,0),2)]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "    glacier_ELAs['quality'] = [1 if (row['ela_ideal_min'] <= row['ela'] <= row['ela_ideal_max']) else 0 for idx,row in glacier_ELAs.iterrows()]\n",
    "\n",
    "    ### get rolling median of quality ela obs in prior x days\n",
    "    x_days = 30\n",
    "\n",
    "    # define function to do this\n",
    "    def get_rolling_median(df_obs, col_name, n_days, min_periods=1, center=False, closed='left'):\n",
    "        temp_df = df_obs[['time',col_name]].set_index('time')\n",
    "        medians = temp_df.rolling(f'{n_days}D', min_periods=min_periods, center=center, closed=closed).median()\n",
    "        return medians\n",
    "\n",
    "    # subset df to good obs\n",
    "    glacier_ELAs_good = glacier_ELAs[glacier_ELAs['quality']==1].copy()\n",
    "    test = get_rolling_median(glacier_ELAs_good, 'ela', x_days, center=False)\n",
    "    glacier_ELAs_good['ela_rolling'] = test.values\n",
    "    glacier_ELAs_good['ela_diff'] = glacier_ELAs_good['ela']-glacier_ELAs_good['ela_rolling']\n",
    "\n",
    "    # lets say that an ela that is 400 or more meter above the rolling median is bad\n",
    "    new_df = glacier_ELAs_good[glacier_ELAs_good['ela_diff']<400]\n",
    "    \n",
    "    print(\"ELAs made\")\n",
    "\n",
    "    # so now from this, lets extract the snow distribution at the end of each year\n",
    "    df = new_df.copy()\n",
    "    df['time_index'] = df['time']\n",
    "    df = df.set_index('time_index')\n",
    "\n",
    "    ys = [2018,2019,2020,2021,2022]\n",
    "    target = 'ela'\n",
    "    data = []\n",
    "    for y in ys:\n",
    "\n",
    "        # subset df to this year\n",
    "        df_subset = df.loc[f'{y}-07-01':f'{y}-11-01']\n",
    "\n",
    "        # get the row that has max ela within this time frame\n",
    "        idx = df_subset[target].idxmax()\n",
    "        ela_max = df_subset.loc[idx]\n",
    "\n",
    "        data.append(ela_max)\n",
    "\n",
    "    max_elas = pd.DataFrame(data)\n",
    "    median_ela = np.nanmean(max_elas[target])\n",
    "\n",
    "    # now go through and grab the snow distributions for these dates\n",
    "    all_maps = []\n",
    "    for d in range(len(data)):\n",
    "        series = data[d]\n",
    "        snow_map = snow.sel(time=series['time'].to_pydatetime())\n",
    "        all_maps.append(snow_map)\n",
    "\n",
    "    # concat the end of summer obs into a single xr_da, calculate a single average snow cover map\n",
    "    all_maps = xr.concat(all_maps, dim='time')\n",
    "    average_map = all_maps.median('time', skipna=True).astype('uint8')+glacier_mask\n",
    "\n",
    "\n",
    "    ### save products to file\n",
    "\n",
    "    # set folder paths, etc...\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "    path_df_all = os.path.join(folder_save, 'csv', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "    path_xr_all = os.path.join(folder_save, 'Annual AAs', f\"S2_{rgiid}_2018_2022_annual_AAs.nc\")\n",
    "    path_xr_avg = os.path.join(folder_save, 'Average AAs', f\"S2_{rgiid}_2018_2022_average_AA.nc\")\n",
    "    path_tif_avg = os.path.join(folder_save, 'Average AAs', f\"S2_{rgiid}_2018_2022_average_AA.tif\")\n",
    "\n",
    "    ### save csv\n",
    "    save_df = pd.DataFrame(data) # table with annual end-of-summer ela, aar, \n",
    "    save_df.to_csv(path_df_all, index=False, columns=['time','ela','aar'])\n",
    "\n",
    "    ### save the annual AA product, average AA as netcdf\n",
    "    # format metadata and attributes\n",
    "    def format_xr_to_save(xr_da):\n",
    "        xr_da.attrs[\"res\"] = (10,10)\n",
    "        xr_da.attrs[\"crs\"] = \"EPSG:3338\"\n",
    "        xr_da.attrs[\"transform\"] = [10,0,0,0,-10,0]\n",
    "        xr_da.attrs[\"_FillValue\"] = 0\n",
    "        xr_da.attrs[\"long_name\"] = rgiid\n",
    "        xr_da.attrs[\"description\"] = \"0: nan, 1: ablation, 2: accumulation\"\n",
    "        xr_da.name = \"accumulation_area\"\n",
    "\n",
    "        xr_da.x.attrs[\"units\"] = \"meters\"\n",
    "        xr_da.y.attrs[\"units\"] = \"meters\"\n",
    "        xr_da.x.attrs[\"long_name\"] = 'x'\n",
    "        xr_da.y.attrs[\"long_name\"] = 'y'\n",
    "\n",
    "#         xr_da.rio.write_crs(3338, inplace=True)\n",
    "\n",
    "        return xr_da\n",
    "\n",
    "    save_xr_all = format_xr_to_save(all_maps.astype('uint8')+glacier_mask)\n",
    "    save_xr_avg = format_xr_to_save(average_map)\n",
    "    save_tif_avg = save_xr_avg.values\n",
    "\n",
    "    # specify compression/encoding\n",
    "    encoding = {\"accumulation_area\":{\"zlib\": True},}# \"x\":{\"zlib\": False}, \"y\":{\"zlib\": False}}\n",
    "\n",
    "    # save\n",
    "    save_xr_all.to_netcdf(path_xr_all, encoding=encoding)\n",
    "    save_xr_avg.to_netcdf(path_xr_avg, encoding=encoding)\n",
    "\n",
    "    ### save average AA as geotiff\n",
    "    from rasterio.transform import from_origin\n",
    "    transform = from_origin(save_xr_avg.x.values[0]-5,\n",
    "                            save_xr_avg.y.values[0]+5,\n",
    "                            save_xr_avg.x.values[1] - save_xr_avg.x.values[0],\n",
    "                            save_xr_avg.y.values[0] - save_xr_avg.y.values[1])\n",
    "\n",
    "    with rio.open(path_tif_avg, 'w', driver='GTiff', height=save_tif_avg.shape[0], width=save_tif_avg.shape[1],\n",
    "                       count=1, dtype=str(save_tif_avg.dtype), crs=\"EPSG:3338\", transform=transform, compress='ZSTD') as dst:\n",
    "        dst.write(save_tif_avg, 1)\n",
    "        \n",
    "    print(\"everything saved\\n\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
