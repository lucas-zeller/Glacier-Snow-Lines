{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "import snowFun\n",
    "import dask.array\n",
    "import importlib\n",
    "importlib.reload(snowFun)\n",
    "# %matplotlib widget\n",
    "\n",
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53085aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"01_rgi60_Alaska\", \"01_rgi60_Alaska.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0f1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset rgi to single outline, by choosing rgiid or rgi name\n",
    "# Wolverine RGIId: RGI60-01.09162\n",
    "# Gulkana RGIId: RGI60-01.00570\n",
    "\n",
    "rgiid = \"RGI60-01.01731\" # really big glacier\n",
    "# rgiid = \"RGI60-01.09162\" # wolverine\n",
    "rginame = 0\n",
    "\n",
    "# get the rgi id for the given glacier name, if a name has been given\n",
    "if rginame:\n",
    "    rgiid = rgi_gdf[rgi_gdf['Name']==rginame]['RGIId'].values[0]\n",
    "\n",
    "# grab just this rgi geometry and info\n",
    "rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "single_geometry = rgi_single.geometry\n",
    "\n",
    "# single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a071907",
   "metadata": {},
   "source": [
    "## open the raw classification data, merge by date, mask out shadow and cloud areas, remove dates with no usable imagery, and then resave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c380e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the glacier classification, clipping to the glacier outline. note we use dask with \"auto\" chunks\n",
    "# we will rechunk later\n",
    "file_name = f\"S2_{rgiid}_2018-01-01_2023-01-01\"\n",
    "xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\"), chunks='auto').rio.clip(single_geometry, from_disk=True, drop=True).chunk()\n",
    "\n",
    "# load metadata csv, convert date to datetimes\n",
    "meta_fp = os.path.join(folder_meta, f\"{file_name}.csv\")\n",
    "meta_df = pd.read_csv(meta_fp)\n",
    "\n",
    "# format time axis to pandas datetime, like xarray wants\n",
    "datetimes = pd.to_datetime([f\"{str(i)[:4]}-{str(i)[4:6]}-{str(i)[6:]}\" for i in meta_df['date']])\n",
    "xr_class = xr_class.rename({\"band\":\"time\"})\n",
    "xr_class['time'] = datetimes\n",
    "\n",
    "# create quick binary glacier mask of 0 and 1\n",
    "glacier_mask = xr_class.max(dim='time')\n",
    "glacier_mask = (glacier_mask>0)#.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec78942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge images on same day\n",
    "xr_class = xr_class.where(xr_class<20, 0).groupby('time').max('time')\n",
    "\n",
    "# get these merged dates\n",
    "datetimes_merged = xr_class.time.values\n",
    "\n",
    "### create binary mask of useable and unuseable data, use it to mask the xr_class to only this area\n",
    "bad_classes = [5] #class 5 is shadow. we will call these area unusable\n",
    "good_classes = [1,2,3,4,6] #snow,firn,ice,debris,water are usable areas\n",
    "# usable = xr.where( xr_class.isin(good_classes), 1, 0)\n",
    "\n",
    "# count total number of pixels on the glacier surface\n",
    "glacier_pixels = glacier_mask.sum().values \n",
    "\n",
    "# now we can mask out unusable areas in each time step\n",
    "xr_class = xr.where(xr_class.isin(good_classes), xr_class, 0) #.sel(time=good_times) # note we subset to only the good days\n",
    "\n",
    "# count useable pixels on each day\n",
    "count_usable_by_time = np.count_nonzero(xr_class, axis=(1,2))\n",
    "\n",
    "# calculate percent of the glacier surface that is usable on each day\n",
    "percent_usable_by_time = count_usable_by_time/glacier_pixels\n",
    "\n",
    "# now lets throw out days where there is essentially no usable data\n",
    "good_times = (percent_usable_by_time>0.05)\n",
    "\n",
    "# print(xr_class)\n",
    "xr_class = xr_class.sel(time=good_times)\n",
    "# print(xr_class.shape)\n",
    "\n",
    "# at this point, xr_class is 0 off-glacier, 0 in shadow/cloud, and 1-6 in usable areas\n",
    "# save indidivual years at this point\n",
    "folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "\n",
    "# save one year at a time\n",
    "for y in [2018,2019,2020,2021,2022]:\n",
    "    print(y)\n",
    "    path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "\n",
    "    save_xr = xr_class.sel(time=slice(f\"{y-1}-12-01\", f\"{y+1}-01-31\")).astype('uint8').rename('class')\n",
    "\n",
    "    # specify compression/encoding\n",
    "    encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "    # save\n",
    "    save_xr.to_netcdf(path_save, encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f340e15",
   "metadata": {},
   "source": [
    "### Then we'll have to load each year back in to do the rolling calculation of snow, and we can resave over the same file (or resave to new file, if we're not concerned about storage space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1c85cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# doing rolling smoothing one year at a time\n",
    "for y in [2018,2019,2020,2021,2022]:\n",
    "    print(y)\n",
    "    \n",
    "    # open the data with each day being its own chunk\n",
    "    path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "    snow = xr.open_dataset(path_open, chunks={'time': 1, 'y': 'auto', 'x': 'auto'})\n",
    "    \n",
    "    # make 1=snow, 0=ablation, nan=cloud,shadow,off-glacier\n",
    "    snow = snow.where(snow!=0, np.nan).where(snow<=1, 0)\n",
    "\n",
    "    # create new empty dataarray with same x/y shape but 1-day frequency, using dask\n",
    "    time_values = pd.to_datetime(snow.time.values)\n",
    "    new_time_values = pd.date_range(start=time_values.min(), end=time_values.max(), freq='D')\n",
    "\n",
    "    # define chunk size for our data. make each pixel its own chunk (all obs of that pixel through time)\n",
    "    xy_chunk = 100\n",
    "    chunks_new_flat = (1, len(snow.y), len(snow.x)) # initially we will have each day be its own chunk\n",
    "#     chunks_new_flat = {'time':30, 'y':len(snow.y), 'x':len(snow.x)}\n",
    "#     chunks_new_long = (len(new_time_values), xy_chunk, xy_chunk) # before doing rolling() over time axis we will rechunk\n",
    "\n",
    "    # make nan daily cadence array\n",
    "    snow2 = dask.array.full((len(new_time_values), len(snow.y), len(snow.x)), np.nan, chunks=chunks_new_flat)# overlapping={'time': 15})\n",
    "    snow2 = xr.DataArray(snow2, coords={'time': new_time_values, 'y': snow.y, 'x': snow.x},\n",
    "                                dims=('time', 'y', 'x'))\n",
    "\n",
    "    # insert observations into this\n",
    "    snow2 = xr.merge([snow2, snow.rename({'class':'snow'})], compat='override').snow.combine_first(snow2)#.chunk(chunks_new_long)\n",
    "\n",
    "    # now use rolling window mean and then extract the original good obs\n",
    "    snow = (snow2.rolling(time=15, min_periods=1, center=False).mean(skipna=True)).loc[dict(time=time_values)]\n",
    "\n",
    "    # we can drop snow2 at this point I think\n",
    "    snow2=None\n",
    "    \n",
    "    # make to 0, 1, nan\n",
    "    snow = xr.where(snow.isnull(), np.nan, xr.where(snow>=0.5, 1, 0))\n",
    "    \n",
    "    # apply a 2d-convolution filter to smooth out the snow distributions\n",
    "    snow_x = snow.rolling({'x':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "    snow_x = snow_x.rolling({'y':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "    norm_x = (snow.notnull()).rolling({'x':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "    norm_x = norm_x.rolling({'y':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "\n",
    "    snow = snow_x/norm_x # this show what fraction of the 3x3 box around each pixel is snow\n",
    "    snow = xr.where(snow.isnull(), 0, xr.where(snow>=0.5, 2, 1))#.astype('uint8') #nodata=0, ablation=1, snow=2\n",
    "    snow = xr.where(glacier_mask==1, snow, 0) # make sure the off-glacier stuff goes to 0  \n",
    "    \n",
    "    snow_x=None\n",
    "    norm_x=None\n",
    "    \n",
    "    # calculate % useable data on each date, % snow by date\n",
    "    percent_all_by_time = (xr.where(snow>0, 1, 0).sum(dim=['x','y'])/glacier_pixels).load()\n",
    "#     percent_snow_by_time = xr.where(snow>1, 1, 0).sum(dim=['x','y'], skipna=True)/xr.where(snow>=0,1,0).sum(dim=['x','y'])\n",
    "    \n",
    "    # filter out the dates that have less than x% usable data\n",
    "    usable_thresh = 0.85\n",
    "    snow = snow.where((percent_all_by_time>usable_thresh), drop=True)\n",
    "\n",
    "    ### make nodata=0, ablation=1, snow=2\n",
    "#     snow = xr.where(snow.isnull(), 0, snow+1)\n",
    "    \n",
    "    ### save\n",
    "    path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "    save_xr = snow.sel(time=slice(f\"{y}-01-01\", f\"{y}-12-31\")).astype('uint8').rename('class')\n",
    "\n",
    "    # specify compression/encoding\n",
    "    encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "    # save\n",
    "    save_xr.to_netcdf(path_save, encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae8019",
   "metadata": {},
   "source": [
    "### now open each year again, calculate elas, save final products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c83641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "starting ELAs\n",
      "1\n",
      "Execution time: 136.26163959503174 seconds\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5.1\n",
      "5.2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(snowFun)\n",
    "csv_data = []\n",
    "# xr_data = []\n",
    "chunks = {'time':-1, 'y':50, 'x':50}\n",
    "# chunks = {'time':1, 'y':-1, 'x':-1}\n",
    "# chunks = {'time':-1, 'y':10, 'x':10}\n",
    "\n",
    "for y in [2018]: #2018,2019,2020,2021,2022\n",
    "    print(y)\n",
    "    \n",
    "    # open the data. think about how you want to chunk it\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "    path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "#     snow = xr.open_dataset(path_open, chunks={'time':1, 'y':'auto', 'x':'auto'})['class'] # nodata=0, ablation=1, snow=2\n",
    "    snow = xr.open_dataset(path_open, chunks=chunks)['class'] # nodata=0, ablation=1, snow=2\n",
    "    \n",
    "    # make to 0, 1, nan\n",
    "    snow = snow.astype(float)-1\n",
    "    snow = snow.where(snow>=0, np.nan)\n",
    "    \n",
    "    # open dem\n",
    "    xr_dem = snowFun.get_year_DEM(single_geometry, 2013)\n",
    "    \n",
    "#     print(xr_dem.shape)\n",
    "#     print(glacier_mask.shape)\n",
    "#     xr_dem.plot()\n",
    "#     continue\n",
    "    \n",
    "    # calculate the \"ideal\" ela-aar relationship\n",
    "    ideal_ELAs = snowFun.idealized_ELA_AAR(xr_dem, glacier_mask)\n",
    "    \n",
    "    print('starting ELAs')\n",
    "    \n",
    "    # extract ELAs from each time step\n",
    "    glacier_ELAs = snowFun.get_the_ELAs(snow, xr_dem, glacier_mask, step=20, width=1, p_snow=0.5)\n",
    "    # this now has, time, aar, ela_ideal, z(list), and ela(single) as columns\n",
    "    continue\n",
    "    print(\"initial ELAs made\")\n",
    "#     print(glacier_ELAs)\n",
    "    \n",
    "    # lets add aar on to the df as well\\\n",
    "    ### this is calculated in previous function, so let's get it to be returned in glacier_ELAs\n",
    "#     glacier_ELAs['aar'] = (xr.where(snow==1, 1, 0).sum(dim=['x','y'], skipna=True)) / ((snow.notnull()).sum(dim=['x','y']))\n",
    "    \n",
    "    # if ela is above the glacier then we get 9999. below and we get -1\n",
    "    # we can change these to the glacier min or max if we want (buffered by 1)\n",
    "    z_min = np.nanmin(xr_dem.where(xr_dem>0))\n",
    "    z_max = np.nanmax(xr_dem)\n",
    "    glacier_ELAs = glacier_ELAs.replace({'ela': {-1:z_min, 9999:z_max} })\n",
    "    \n",
    "    # lets use this aar-ela relationship to root out bad observations\n",
    "    # for each aar we observed, see what the ideal ela would be\n",
    "    glacier_ELAs['aar_round'] = glacier_ELAs['aar'].round(2)\n",
    "    glacier_ELAs['ela_ideal'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==i]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "\n",
    "    # how about we incorporate a little error and see the range of elas we could expect\n",
    "    error_allowed = 0.2\n",
    "    glacier_ELAs['ela_ideal_min'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==round(min(i+error_allowed,1),2)]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "    glacier_ELAs['ela_ideal_max'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==round(max(i-error_allowed,0),2)]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "    glacier_ELAs['quality'] = [1 if (row['ela_ideal_min'] <= row['ela'] <= row['ela_ideal_max']) else 0 for idx,row in glacier_ELAs.iterrows()]\n",
    "\n",
    "    ###### need to think of better ways to do this. We\n",
    "    ### get rolling median of quality ela obs in prior x days\n",
    "    x_days = 30\n",
    "\n",
    "    # define function to do this\n",
    "    def get_rolling_median(df_obs, col_name, n_days, min_periods=1, center=False, closed='left'):\n",
    "        temp_df = df_obs[['time',col_name]].set_index('time')\n",
    "        medians = temp_df.rolling(f'{n_days}D', min_periods=min_periods, center=center, closed=closed).median()\n",
    "        return medians\n",
    "\n",
    "    # subset df to good obs\n",
    "    glacier_ELAs_good = glacier_ELAs[glacier_ELAs['quality']==1].copy()\n",
    "    test = get_rolling_median(glacier_ELAs_good, 'ela', x_days, center=False)\n",
    "    glacier_ELAs_good['ela_rolling'] = test.values\n",
    "    glacier_ELAs_good['ela_diff'] = glacier_ELAs_good['ela']-glacier_ELAs_good['ela_rolling']\n",
    "    \n",
    "    # lets say that an ela that is 400 or more meter above the rolling median is bad\n",
    "    new_df = glacier_ELAs_good[glacier_ELAs_good['ela_diff']<400]\n",
    "    \n",
    "    print(\"final elas made\")\n",
    "    \n",
    "    ### so now from this, lets extract the snow distribution at the end of each year\n",
    "    df = new_df.copy()\n",
    "    df['time_index'] = df['time']\n",
    "    df = df.set_index('time_index')\n",
    "    \n",
    "    # subset to just the months of interest\n",
    "    df_subset = df.loc[f'{y}-07-01':f'{y}-11-01']\n",
    "    \n",
    "    # if we have valid obs, then we take the highest ela\n",
    "    if len(df_subset)>0:\n",
    "        \n",
    "        # get the row that has max ela within this time frame\n",
    "        idx = df_subset['ela'].idxmax()\n",
    "        ela_max = df_subset.loc[idx]\n",
    "        ela_max['quality'] = 1\n",
    "\n",
    "        # get the snow map that is on this date\n",
    "        snow_map = snow.sel(time=ela_max['time'].to_pydatetime())\n",
    "\n",
    "        # append to save\n",
    "        csv_data.append(ela_max)\n",
    "#         xr_data.append(snow_map)\n",
    "    \n",
    "    else: # if we have no valid obs, then set a null row in the csv data and carry on\n",
    "        csv_data.append(pd.Series({\"time\":f\"01/01/{y}\", \"ela\":-9999, 'aar':-9999, 'quality':0})) # append row of empty/null values to the df\n",
    "        \n",
    "\n",
    "save = 1\n",
    "if save:\n",
    "    # first we save the csv with dates, elas, aars, etc...\n",
    "    # then we use that info to select the snow cover product from each year and save it\n",
    "    \n",
    "\n",
    "    # set folder paths, etc...\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "    # path_df_all = os.path.join(folder_save, 'csv', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "    # path_xr_all = os.path.join(folder_save, 'Annual AAs', f\"S2_{rgiid}_2018_2022_annual_AAs.nc\")\n",
    "    # path_xr_avg = os.path.join(folder_save, 'Average AAs', f\"S2_{rgiid}_2018_2022_average_AA.nc\")\n",
    "    # path_tif_avg = os.path.join(folder_save, 'Average AAs', f\"S2_{rgiid}_2018_2022_average_AA.tif\")\n",
    "    path_df_all = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "    path_xr_all = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_annual_AAs.nc\")\n",
    "    path_xr_avg = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_average_AA.nc\")\n",
    "    path_tif_avg = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_average_AA.tif\")\n",
    "\n",
    "    ### format and save csv\n",
    "    max_elas = pd.DataFrame(csv_data)\n",
    "    max_elas.to_csv(path_df_all, index=False, columns=['time','ela','aar']) # table with annual end-of-summer ela, aar, \n",
    "\n",
    "    # see how many of the years had a useable product\n",
    "    n_usable = max_elas['quality'].sum()\n",
    "\n",
    "    ### format and save maps, if we have valid observations\n",
    "    # function to format metadata and attributes\n",
    "    def format_xr_to_save(xr_da):\n",
    "        xr_da.attrs[\"res\"] = (10,10)\n",
    "        xr_da.attrs[\"crs\"] = \"EPSG:3338\"\n",
    "        xr_da.attrs[\"transform\"] = [10,0,0,0,-10,0]\n",
    "        xr_da.attrs[\"_FillValue\"] = 0\n",
    "        xr_da.attrs[\"long_name\"] = rgiid\n",
    "        xr_da.attrs[\"description\"] = \"0: nan, 1: ablation, 2: accumulation\"\n",
    "        xr_da.name = \"accumulation_area\"\n",
    "\n",
    "        xr_da.x.attrs[\"units\"] = \"meters\"\n",
    "        xr_da.y.attrs[\"units\"] = \"meters\"\n",
    "        xr_da.x.attrs[\"long_name\"] = 'x'\n",
    "        xr_da.y.attrs[\"long_name\"] = 'y'\n",
    "\n",
    "        return xr_da\n",
    "\n",
    "    if n_usable>0:\n",
    "        \n",
    "        dates = max_elas['time']\n",
    "        dates = [d.to_pydatetime() for d in dates if d.month!=\"1\"] # toss out the \"bad ones\"\n",
    "        \n",
    "        all_maps = []\n",
    "        \n",
    "        for d in dates:\n",
    "            y = d.year\n",
    "            print(y)\n",
    "        \n",
    "            # reopen the daily snow data\n",
    "            path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "            snow = xr.open_dataset(path_open, chunks={'time':1})['class'].sel(time=d)\n",
    "            all_maps.append(snow)\n",
    "        \n",
    "        # format to save\n",
    "        all_maps = xr.concat(all_maps, dim='time')\n",
    "        average_map = all_maps.median('time', skipna=True).astype('uint8')+glacier_mask\n",
    "        save_xr_all = format_xr_to_save(all_maps.astype('uint8')+glacier_mask)\n",
    "        save_xr_avg = format_xr_to_save(average_map)\n",
    "        \n",
    "        # specify compression/encoding\n",
    "        encoding = {\"accumulation_area\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "        # save\n",
    "        save_xr_all.to_netcdf(path_xr_all, encoding=encoding)\n",
    "        save_xr_avg.to_netcdf(path_xr_avg, encoding=encoding)\n",
    "        save_xr_avg.rio.to_raster(raster_path=path_tif_avg, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5cb994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
