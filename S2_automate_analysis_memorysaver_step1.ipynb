{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import snowFun\n",
    "import dask.array\n",
    "# %matplotlib widget\n",
    "\n",
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"01_rgi60_Alaska\", \"01_rgi60_Alaska.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate through glaciers, running the analysis for each\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# select which rgis to analyze\n",
    "# rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "# rgis_to_analyze = rgis_folder # everything that is available\n",
    "rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# sort\n",
    "rgis_to_analyze.sort()\n",
    "# print(rgis_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d78c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting 281 of 483: RGI60-01.10594  3.568 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Initial daily AAs made\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Smoothed daily AAs made\n",
      "starting ELAs 2018\n",
      "starting ELAs 2019\n",
      "starting ELAs 2020\n",
      "starting ELAs 2021\n",
      "starting ELAs 2022\n",
      "Saving results\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "\n",
      "Starting 282 of 483: RGI60-01.10607  121.571 km2\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "Initial daily AAs made\n",
      "2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzell\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzell\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzell\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzell\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\indexing.py:1379: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n",
      "Smoothed daily AAs made\n",
      "starting ELAs 2018\n",
      "starting ELAs 2019\n",
      "starting ELAs 2020\n",
      "starting ELAs 2021\n",
      "starting ELAs 2022\n",
      "Saving results\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "\n",
      "Starting 283 of 483: RGI60-01.10612  152.284 km2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m xr_class \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mwhere(xr_class\u001b[38;5;241m.\u001b[39misin(good_classes), xr_class, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#.sel(time=good_times) # note we subset to only the good days\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# count useable pixels on each day\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m count_usable_by_time \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_nonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxr_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# calculate percent of the glacier surface that is usable on each day\u001b[39;00m\n\u001b[0;32m     69\u001b[0m percent_usable_by_time \u001b[38;5;241m=\u001b[39m count_usable_by_time\u001b[38;5;241m/\u001b[39mglacier_pixels\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mcount_nonzero\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\numpy\\core\\numeric.py:495\u001b[0m, in \u001b[0;36mcount_nonzero\u001b[1;34m(a, axis, keepdims)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keepdims:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m multiarray\u001b[38;5;241m.\u001b[39mcount_nonzero(a)\n\u001b[1;32m--> 495\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m# TODO: this works around .astype(bool) not working properly (gh-9847)\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(a\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mcharacter):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\common.py:165\u001b[0m, in \u001b[0;36mAbstractArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\dataarray.py:738\u001b[0m, in \u001b[0;36mDataArray.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    731\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;124;03m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;124;03m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\variable.py:607\u001b[0m, in \u001b[0;36mVariable.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    606\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\xarray\\core\\variable.py:313\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\dask\\array\\core.py:1701\u001b[0m, in \u001b[0;36mArray.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1701\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[0;32m   1703\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\dask\\base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\dask\\base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    597\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 599\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\dask\\local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[1;32m--> 500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m    502\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\site-packages\\dask\\local.py:130\u001b[0m, in \u001b[0;36mqueue_get\u001b[1;34m(q)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AGVA_env\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skip = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "    \n",
    "    # options for skipping\n",
    "    if (i+1)<283:continue #option to skip, NEED TO GO BACK TO 241, 263, 277, 280\n",
    "    \n",
    "#     if rgiid == \"RGI60-01.09162\": skip=0\n",
    "#     if skip: continue\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area']\n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga.values[0]} km2\")\n",
    "\n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    "    \n",
    "    # open the classification data\n",
    "    file_name = f\"S2_{rgiid}_2018-01-01_2023-01-01\"\n",
    "    xr_class = riox.open_rasterio(os.path.join(folder_class, f\"{file_name}.tif\")).chunk({'band':-1, 'y':1000, 'x':1000})#.rio.clip(single_geometry, from_disk=True, drop=True).chunk()\n",
    "\n",
    "    \n",
    "#     #set 9999 to 0\n",
    "#     xr_class = xr.where(xr_class>90,0,xr_class)\n",
    "    \n",
    "    # load metadata csv, convert date to datetimes\n",
    "    meta_fp = os.path.join(folder_meta, f\"{file_name}.csv\")\n",
    "    meta_df = pd.read_csv(meta_fp)\n",
    "\n",
    "    # format time axis to pandas datetime, like xarray wants\n",
    "    datetimes = pd.to_datetime([f\"{str(i)[:4]}-{str(i)[4:6]}-{str(i)[6:]}\" for i in meta_df['date']])\n",
    "    xr_class = xr_class.rename({\"band\":\"time\"})\n",
    "    xr_class['time'] = datetimes\n",
    "\n",
    "\n",
    "    # merge images on same day\n",
    "    xr_class = xr_class.where(xr_class<20, 0).groupby('time').max('time')\n",
    "\n",
    "    # get these merged dates\n",
    "    datetimes_merged = xr_class.time.values\n",
    "\n",
    "    ### create binary mask of useable and unuseable data, use it to mask the xr_class to only this area\n",
    "    bad_classes = [5] #class 5 is shadow. we will call these area unusable\n",
    "    good_classes = [1,2,3,4,6] #snow,firn,ice,debris,water are usable areas\n",
    "    # usable = xr.where( xr_class.isin(good_classes), 1, 0)\n",
    "\n",
    "    # create quick binary glacier mask of 0 and 1\n",
    "    glacier_mask = xr_class.max(dim='time')\n",
    "    glacier_mask = (glacier_mask>0)#.astype(int)\n",
    "\n",
    "    # count total number of pixels on the glacier surface, based on the glacier rgi area\n",
    "    glacier_pixels = int(ga * (1000*1000) / (10*10))\n",
    "#     glacier_pixels = glacier_mask.sum().values \n",
    "\n",
    "\n",
    "    # now we can mask out unusable areas in each time step\n",
    "    xr_class = xr.where(xr_class.isin(good_classes), xr_class, 0) #.sel(time=good_times) # note we subset to only the good days\n",
    "\n",
    "    # count useable pixels on each day\n",
    "    count_usable_by_time = np.count_nonzero(xr_class, axis=(1,2))\n",
    "\n",
    "    # calculate percent of the glacier surface that is usable on each day\n",
    "    percent_usable_by_time = count_usable_by_time/glacier_pixels\n",
    "\n",
    "    # now lets throw out days where there is essentially no usable data\n",
    "    good_times = (percent_usable_by_time>0.05)\n",
    "\n",
    "    # print(xr_class)\n",
    "    xr_class = xr_class.sel(time=good_times)\n",
    "    # print(xr_class.shape)\n",
    "\n",
    "    # at this point, xr_class is 0 off-glacier, 0 in shadow/cloud, and 1-6 in usable areas\n",
    "    # save indidivual years at this point\n",
    "    folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "\n",
    "    # save one year at a time\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        print(y)\n",
    "        path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "\n",
    "        save_xr = xr_class.sel(time=slice(f\"{y-1}-12-01\", f\"{y+1}-01-31\")).astype('uint8').rename('class')\n",
    "\n",
    "        # specify compression/encoding\n",
    "        encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "        # save\n",
    "        save_xr.to_netcdf(path_save, encoding=encoding)\n",
    "    \n",
    "    print(\"Initial daily AAs made\")\n",
    "    \n",
    "    # doing rolling smoothing one year at a time\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        print(y)\n",
    "\n",
    "        # open the data with each day being its own chunk\n",
    "        path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "        snow = xr.open_dataset(path_open, chunks={'time': 1, 'y': 'auto', 'x': 'auto'})\n",
    "\n",
    "        # make 1=snow, 0=ablation, nan=cloud,shadow,off-glacier\n",
    "        snow = snow.where(snow!=0, np.nan).where(snow<=1, 0)\n",
    "\n",
    "        # create new empty dataarray with same x/y shape but 1-day frequency, using dask\n",
    "        time_values = pd.to_datetime(snow.time.values)\n",
    "        new_time_values = pd.date_range(start=time_values.min(), end=time_values.max(), freq='D')\n",
    "\n",
    "        # define chunk size for our data. make each pixel its own chunk (all obs of that pixel through time)\n",
    "        xy_chunk = 100\n",
    "        chunks_new_flat = (1, len(snow.y), len(snow.x)) # initially we will have each day be its own chunk\n",
    "    #     chunks_new_flat = {'time':30, 'y':len(snow.y), 'x':len(snow.x)}\n",
    "    #     chunks_new_long = (len(new_time_values), xy_chunk, xy_chunk) # before doing rolling() over time axis we will rechunk\n",
    "\n",
    "        # make nan daily cadence array\n",
    "        snow2 = dask.array.full((len(new_time_values), len(snow.y), len(snow.x)), np.nan, chunks=chunks_new_flat)# overlapping={'time': 15})\n",
    "        snow2 = xr.DataArray(snow2, coords={'time': new_time_values, 'y': snow.y, 'x': snow.x},\n",
    "                                    dims=('time', 'y', 'x'))\n",
    "\n",
    "        # insert observations into this\n",
    "        snow2 = xr.merge([snow2, snow.rename({'class':'snow'})], compat='override').snow.combine_first(snow2)#.chunk(chunks_new_long)\n",
    "\n",
    "        # now use rolling window mean and then extract the original good obs\n",
    "        snow = (snow2.rolling(time=15, min_periods=1, center=False).mean(skipna=True)).loc[dict(time=time_values)]\n",
    "\n",
    "        # we can drop snow2 at this point I think\n",
    "        snow2=None\n",
    "\n",
    "        # make to 0, 1, nan\n",
    "        snow = xr.where(snow.isnull(), np.nan, xr.where(snow>=0.5, 1, 0))\n",
    "\n",
    "        # apply a 2d-convolution filter to smooth out the snow distributions\n",
    "        snow_x = snow.rolling({'x':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "        snow_x = snow_x.rolling({'y':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "        norm_x = (snow.notnull()).rolling({'x':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "        norm_x = norm_x.rolling({'y':3}, min_periods=1, center=True).sum(skipna=True)\n",
    "\n",
    "        snow = snow_x/norm_x # this show what fraction of the 3x3 box around each pixel is snow\n",
    "        snow = xr.where(snow.isnull(), 0, xr.where(snow>=0.5, 2, 1))#.astype('uint8') #nodata=0, ablation=1, snow=2\n",
    "        snow = xr.where(glacier_mask==1, snow, 0) # make sure the off-glacier stuff goes to 0  \n",
    "\n",
    "        snow_x=None\n",
    "        norm_x=None\n",
    "\n",
    "        # calculate % useable data on each date, % snow by date\n",
    "        percent_all_by_time = (xr.where(snow>0, 1, 0).sum(dim=['x','y'])/glacier_pixels)#.load()\n",
    "    #     percent_snow_by_time = xr.where(snow>1, 1, 0).sum(dim=['x','y'], skipna=True)/xr.where(snow>=0,1,0).sum(dim=['x','y'])\n",
    "\n",
    "        # filter out the dates that have less than x% usable data\n",
    "        usable_thresh = 0.85\n",
    "        snow = snow.where((percent_all_by_time>usable_thresh), drop=True)\n",
    "\n",
    "        ### make nodata=0, ablation=1, snow=2\n",
    "    #     snow = xr.where(snow.isnull(), 0, snow+1)\n",
    "\n",
    "        ### save\n",
    "        path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "        save_xr = snow.sel(time=slice(f\"{y}-01-01\", f\"{y}-12-31\")).astype('uint8').rename('class')\n",
    "\n",
    "        # specify compression/encoding\n",
    "        encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "        # save\n",
    "        save_xr.to_netcdf(path_save, encoding=encoding)\n",
    "    \n",
    "    print(\"Smoothed daily AAs made\")\n",
    "    \n",
    "    ### lastly calculate elas for each year, pick the best, save all our products\n",
    "    csv_data = []\n",
    "    # xr_data = []\n",
    "    chunks = {'time':-1, 'y':50, 'x':50}\n",
    "    # chunks = {'time':1, 'y':-1, 'x':-1}\n",
    "    # chunks = {'time':-1, 'y':10, 'x':10}\n",
    "\n",
    "    for y in [2018,2019,2020,2021,2022]: #2018,2019,2020,2021,2022\n",
    "#         print(y)\n",
    "\n",
    "        # open the data. think about how you want to chunk it\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "        path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "    #     snow = xr.open_dataset(path_open, chunks={'time':1, 'y':'auto', 'x':'auto'})['class'] # nodata=0, ablation=1, snow=2\n",
    "        snow = xr.open_dataset(path_open, chunks=chunks)['class'] # nodata=0, ablation=1, snow=2\n",
    "\n",
    "        # make to 0, 1, nan\n",
    "        snow = snow.astype(float)-1\n",
    "        snow = snow.where(snow>=0, np.nan)\n",
    "\n",
    "        # open dem\n",
    "        xr_dem = snowFun.get_year_DEM(single_geometry, 2013)\n",
    "\n",
    "        \n",
    "        # sometimes the dem is wider than the glacier mask/data. we need to drop the extra x/y data\n",
    "        # lets hope we never get a time where the mask is larger than the dem...\n",
    "#         if len(xr_dem.x)!=len(glacier_mask.x):\n",
    "#             xr_dem = xr_dem.sel(x=slice(min(glacier_mask.x),max(glacier_mask.x)), y=slice(min(glacier_mask.y),max(glacier_mask.y)))\n",
    "#         if len(xr_dem.y)!=len(glacier_mask.y):\n",
    "#             xr_dem = xr_dem.sel(y=slice(min(glacier_mask.y),max(glacier_mask.y)))\n",
    "        \n",
    "#         print(min(xr_dem.x.values),max(xr_dem.x.values),min(xr_dem.y.values),max(xr_dem.y.values) )\n",
    "#         print(min(glacier_mask.x.values),max(glacier_mask.x.values),min(glacier_mask.y.values),max(glacier_mask.y.values) )\n",
    "#         print(xr_dem.shape)\n",
    "#         print(glacier_mask.shape)\n",
    "#         print(snow.shape)\n",
    "        \n",
    "        xr_dem = xr_dem.sel(x=slice( min(glacier_mask.x.values), max(glacier_mask.x.values) ), y=slice(max(glacier_mask.y.values),min(glacier_mask.y.values)))\n",
    "        \n",
    "#         print(xr_dem.shape)\n",
    "#         print(glacier_mask.shape)\n",
    "    #     xr_dem.plot()\n",
    "    #     continue\n",
    "\n",
    "            \n",
    "        # calculate the \"ideal\" ela-aar relationship\n",
    "        ideal_ELAs = snowFun.idealized_ELA_AAR(xr_dem, glacier_mask)\n",
    "\n",
    "        print(f'starting ELAs {y}')\n",
    "\n",
    "        # extract ELAs from each time step\n",
    "        glacier_ELAs = snowFun.get_the_ELAs(snow, xr_dem, glacier_mask, step=20, width=1, p_snow=0.5)\n",
    "        # this now has, time, aar, ela_ideal, z(list), and ela(single) as columns\n",
    "#         continue\n",
    "#         print(\"initial ELAs made\")\n",
    "    #     print(glacier_ELAs)\n",
    "\n",
    "        # lets add aar on to the df as well\\\n",
    "        ### this is calculated in previous function, so let's get it to be returned in glacier_ELAs\n",
    "    #     glacier_ELAs['aar'] = (xr.where(snow==1, 1, 0).sum(dim=['x','y'], skipna=True)) / ((snow.notnull()).sum(dim=['x','y']))\n",
    "\n",
    "        # if ela is above the glacier then we get 9999. below and we get -1\n",
    "        # we can change these to the glacier min or max if we want (buffered by 1)\n",
    "        z_min = np.nanmin(xr_dem.where(xr_dem>0))\n",
    "        z_max = np.nanmax(xr_dem)\n",
    "        glacier_ELAs = glacier_ELAs.replace({'ela': {-1:z_min, 9999:z_max} })\n",
    "\n",
    "        # lets use this aar-ela relationship to root out bad observations\n",
    "        # for each aar we observed, see what the ideal ela would be\n",
    "        glacier_ELAs['aar_round'] = glacier_ELAs['aar'].round(2)\n",
    "        glacier_ELAs['ela_ideal'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==i]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "\n",
    "        # how about we incorporate a little error and see the range of elas we could expect\n",
    "        error_allowed = 0.2\n",
    "        glacier_ELAs['ela_ideal_min'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==round(min(i+error_allowed,1),2)]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "        glacier_ELAs['ela_ideal_max'] = [ ideal_ELAs[ideal_ELAs['aar'].round(2)==round(max(i-error_allowed,0),2)]['ela'].values[0] for i in glacier_ELAs['aar_round'] ]\n",
    "        glacier_ELAs['quality'] = [1 if (row['ela_ideal_min'] <= row['ela'] <= row['ela_ideal_max']) else 0 for idx,row in glacier_ELAs.iterrows()]\n",
    "\n",
    "        ###### need to think of better ways to do this. We\n",
    "        ### get rolling median of quality ela obs in prior x days\n",
    "        x_days = 30\n",
    "\n",
    "        # define function to do this\n",
    "        def get_rolling_median(df_obs, col_name, n_days, min_periods=1, center=False, closed='left'):\n",
    "            temp_df = df_obs[['time',col_name]].set_index('time')\n",
    "            medians = temp_df.rolling(f'{n_days}D', min_periods=min_periods, center=center, closed=closed).median()\n",
    "            return medians\n",
    "\n",
    "        # subset df to good obs\n",
    "        glacier_ELAs_good = glacier_ELAs[glacier_ELAs['quality']==1].copy()\n",
    "        test = get_rolling_median(glacier_ELAs_good, 'ela', x_days, center=False)\n",
    "        glacier_ELAs_good['ela_rolling'] = test.values\n",
    "        glacier_ELAs_good['ela_diff'] = glacier_ELAs_good['ela']-glacier_ELAs_good['ela_rolling']\n",
    "\n",
    "        # lets say that an ela that is 400 or more meter above the rolling median is bad\n",
    "        new_df = glacier_ELAs_good[glacier_ELAs_good['ela_diff']<400]\n",
    "\n",
    "#         print(\"final elas made\")\n",
    "\n",
    "        ### so now from this, lets extract the snow distribution at the end of each year\n",
    "        df = new_df.copy()\n",
    "        df['time_index'] = df['time']\n",
    "        df = df.set_index('time_index')\n",
    "\n",
    "        # subset to just the months of interest\n",
    "        df_subset = df.loc[f'{y}-07-01':f'{y}-11-01']\n",
    "\n",
    "        # if we have valid obs, then we take the highest ela\n",
    "        if len(df_subset)>0:\n",
    "\n",
    "            # get the row that has max ela within this time frame\n",
    "            idx = df_subset['ela'].idxmax()\n",
    "            ela_max = df_subset.loc[idx]\n",
    "#             print(ela_max)\n",
    "#             ela_max['quality'] = 1\n",
    "\n",
    "            # get the snow map that is on this date\n",
    "            snow_map = snow.sel(time=ela_max['time'].to_pydatetime())\n",
    "\n",
    "            # append to save\n",
    "            csv_data.append(ela_max)\n",
    "    #         xr_data.append(snow_map)\n",
    "\n",
    "        else: # if we have no valid obs, then set a null row in the csv data and carry on\n",
    "            csv_data.append(pd.Series({\"time\":datetime.strptime(f\"01/01/{y}\", \"%d/%m/%Y\"), \"ela\":-9999, 'aar':-9999, 'quality':0})) # append row of empty/null values to the df\n",
    "\n",
    "\n",
    "    save = 1\n",
    "    if save:\n",
    "        print(\"Saving results\")\n",
    "        \n",
    "        # first we save the csv with dates, elas, aars, etc...\n",
    "        # then we use that info to select the snow cover product from each year and save it\n",
    "        \n",
    "        # set folder paths, etc...\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "        path_df_all = os.path.join(folder_save, 'csv', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "        path_xr_all = os.path.join(folder_save, 'Annual AAs', f\"S2_{rgiid}_2018_2022_annual_AAs.nc\")\n",
    "        path_xr_avg = os.path.join(folder_save, 'Average AAs', f\"S2_{rgiid}_2018_2022_average_AA.nc\")\n",
    "        path_tif_avg = os.path.join(folder_save, 'Average AAs', f\"S2_{rgiid}_2018_2022_average_AA.tif\")\n",
    "#         path_df_all = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_annual_AAs.csv\")\n",
    "#         path_xr_all = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_annual_AAs.nc\")\n",
    "#         path_xr_avg = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_average_AA.nc\")\n",
    "#         path_tif_avg = os.path.join(folder_save, 'temp', f\"S2_{rgiid}_2018_2022_average_AA.tif\")\n",
    "\n",
    "        ### format and save csv\n",
    "        max_elas = pd.DataFrame(csv_data)\n",
    "        max_elas.to_csv(path_df_all, index=False, columns=['time','ela','aar']) # table with annual end-of-summer ela, aar, \n",
    "\n",
    "        # see how many of the years had a useable product\n",
    "        n_usable = max_elas['quality'].sum()\n",
    "\n",
    "        ### format and save maps, if we have valid observations\n",
    "        # function to format metadata and attributes\n",
    "        def format_xr_to_save(xr_da):\n",
    "            xr_da.attrs[\"res\"] = (10,10)\n",
    "            xr_da.attrs[\"crs\"] = \"EPSG:3338\"\n",
    "            xr_da.attrs[\"transform\"] = [10,0,0,0,-10,0]\n",
    "            xr_da.attrs[\"_FillValue\"] = 0\n",
    "            xr_da.attrs[\"long_name\"] = rgiid\n",
    "            xr_da.attrs[\"description\"] = \"0: nan, 1: ablation, 2: accumulation\"\n",
    "            xr_da.name = \"accumulation_area\"\n",
    "\n",
    "            xr_da.x.attrs[\"units\"] = \"meters\"\n",
    "            xr_da.y.attrs[\"units\"] = \"meters\"\n",
    "            xr_da.x.attrs[\"long_name\"] = 'x'\n",
    "            xr_da.y.attrs[\"long_name\"] = 'y'\n",
    "\n",
    "            return xr_da\n",
    "\n",
    "        if n_usable>0:\n",
    "\n",
    "            dates = max_elas['time']\n",
    "#             print(dates)\n",
    "#             print( [d.month for d in dates])\n",
    "            dates = [d.to_pydatetime() for d in dates if d.month!=1] # toss out the \"bad ones\"\n",
    "#             print(dates)\n",
    "            \n",
    "            all_maps = []\n",
    "\n",
    "            for d in dates:\n",
    "                y = d.year\n",
    "                print(y)\n",
    "\n",
    "                # reopen the daily snow data\n",
    "                path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "                snow = xr.open_dataset(path_open, chunks={'time':1})['class'].sel(time=d)\n",
    "                all_maps.append(snow)\n",
    "\n",
    "            # format to save\n",
    "            all_maps = xr.concat(all_maps, dim='time')\n",
    "            average_map = all_maps.median('time', skipna=True).astype('uint8')+glacier_mask\n",
    "            save_xr_all = format_xr_to_save(all_maps.astype('uint8')+glacier_mask)\n",
    "            save_xr_avg = format_xr_to_save(average_map)\n",
    "\n",
    "            # specify compression/encoding\n",
    "            encoding = {\"accumulation_area\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "            # save\n",
    "            save_xr_all.to_netcdf(path_xr_all, encoding=encoding)\n",
    "            save_xr_avg.to_netcdf(path_xr_avg, encoding=encoding)\n",
    "            save_xr_avg.rio.to_raster(raster_path=path_tif_avg, encoding=encoding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
