{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d945ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as riox\n",
    "import rasterio as rio\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from osgeo import gdal\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import snowFun\n",
    "import dask.array\n",
    "# %matplotlib widget\n",
    "\n",
    "# define folder and file paths\n",
    "folder_AGVA = os.path.join('C:',os.sep,'Users','lzell','OneDrive - Colostate','Desktop',\"AGVA\")\n",
    "folder_dems = os.path.join(folder_AGVA, \"DEMs\", \"time_varying_DEMs\", \"10m\")\n",
    "folder_class = os.path.join(folder_AGVA, 'classified images', 'S2_Classified_Cloudmasked_Merged')\n",
    "folder_cloud = os.path.join(folder_AGVA, 'classified images', 'S2_Cloud_Merged')\n",
    "folder_meta = os.path.join(folder_AGVA, \"classified images\", \"meta csv\", \"S2\")\n",
    "folder_mask = os.path.join(folder_AGVA, 'Derived products', 'S2', 'Masks')\n",
    "\n",
    "# open rgi\n",
    "path_rgi = os.path.join(folder_AGVA, 'RGI', \"01_rgi60_Alaska\", \"01_rgi60_Alaska.shp\")\n",
    "rgi_gdf = gpd.read_file(path_rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "### iterate through glaciers, running the analysis for each\n",
    "# get rgi names for given o2 region\n",
    "rgis_o2 = rgi_gdf[rgi_gdf['O2Region']=='4']['RGIId'].values\n",
    "\n",
    "# load rgi names that have been saved to the classified folder\n",
    "rgis_folder = list(set( [ i[3:17] for i in os.listdir(folder_class) if i!='merged.vrt' ] ))\n",
    "\n",
    "# select which rgis to analyze\n",
    "# rgis_to_analyze = [\"RGI60-01.09162\"] # just a single rgi\n",
    "# rgis_to_analyze = rgis_folder # everything that is available\n",
    "rgis_to_analyze = list( set(rgis_folder).intersection(set(rgis_o2)) ) # all the rgis in the folder than are in this o2region\n",
    "\n",
    "# sort\n",
    "rgis_to_analyze.sort()\n",
    "# print(rgis_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d78c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 0\n",
    "for i in range(len(rgis_to_analyze)):\n",
    "    \n",
    "    # subset rgi to single outline, by choosing rgiid or rgi name\n",
    "    rgiid = rgis_to_analyze[i]\n",
    "    \n",
    "    # agregatted needs: 277, 280, 290\n",
    "#     if (i+1) not in [405,406,407,408,420, 241, 277, 280, 290, 291]: continue\n",
    "    if (i+1) not in [241, 277, 280, 290]: continue\n",
    "    # options for skipping\n",
    "#     if (i+1)<296: continue ### just need 281-400, as well as 405-408,420, (290 just for this)\n",
    "#     if (i+1)>325: continue\n",
    "     #everything done to here NEED TO GO BACK TO 405, 406, 407, 408, 420\n",
    "        \n",
    "#     if (i+1)<359: continue ### just need 281-400, as well as 405-408,420\n",
    "#     if (i+1)>400: continue #option to skip, NEED TO GO BACK TO 241, 277, 280\n",
    "#     \n",
    "#     if (i+1)!=358: continue\n",
    "    \n",
    "#     if rgiid == \"RGI60-01.09162\": skip=0\n",
    "#     if skip: continue\n",
    "\n",
    "    # quickly grab glacier area\n",
    "    ga = rgi_gdf[rgi_gdf['RGIId']==rgiid]['Area']\n",
    "    # print progress\n",
    "    print(f\"\\nStarting {i+1} of {len(rgis_to_analyze)}: {rgiid}  {ga.values[0]} km2\")\n",
    "    \n",
    "    # grab just this rgi geometry and info\n",
    "    rgi_single = rgi_gdf[rgi_gdf['RGIId']==rgiid].to_crs(\"EPSG:3338\")\n",
    "    single_geometry = rgi_single.geometry\n",
    "\n",
    "    # single_geometry = single_geometry.buffer(-100) #what if we buffer out the exterior 100 meters of the glacier\n",
    " \n",
    "    # open glacier mask\n",
    "    glacier_mask = xr.open_dataset(os.path.join(folder_mask, f\"S2_{rgiid}_mask.nc\"), chunks='auto') \n",
    "    \n",
    "    # count total number of pixels on the glacier surface, based on the glacier rgi area or on glacier mask\n",
    "#     glacier_pixels = glacier_mask.sum().values\n",
    "    glacier_pixels = int(ga * (1000*1000) / (10*10))\n",
    "    \n",
    "    # doing rolling smoothing one year at a time\n",
    "    for y in [2018,2019,2020,2021,2022]:\n",
    "        print(y)\n",
    "\n",
    "        # open the data with each day being its own chunk\n",
    "        folder_save = os.path.join(folder_AGVA, 'Derived products', 'S2')\n",
    "        path_open = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs.nc\")\n",
    "        snow = xr.open_dataset(path_open, chunks={'time': 1, 'y': -1, 'x': -1})\n",
    "    \n",
    "        # make 1=snow, 0=ablation, nan=cloud,shadow,off-glacier\n",
    "        snow = snow.where(snow!=0, np.nan).where(snow<=1, 0)\n",
    "        \n",
    "        # get list of the date of every observation\n",
    "        time_values = pd.to_datetime(snow.time.values)\n",
    "        \n",
    "        # now iterate through each date. get all obs from up to 15 days before, average through time\n",
    "        all_smoothed = []\n",
    "        for i in range(len(time_values)):\n",
    "#             print(i)\n",
    "            # grab this date, calculate difference\n",
    "            date = time_values[i]\n",
    "            diffs = time_values-date\n",
    "            \n",
    "            # find which dates are in the preceding 15 dates\n",
    "            good_dates = time_values[(diffs>=\"-15d\") & (diffs<=\"0d\")]\n",
    "#             print(date, good_dates)\n",
    "            \n",
    "            # select obs from these dates, take average\n",
    "            smoothed = snow.sel(time=good_dates).mean(dim='time', skipna=True)['class']\n",
    "#             print(smoothed.head)\n",
    "            \n",
    "            # fix to 0(nodata), 1(ablation), 2(snow)\n",
    "            smoothed = xr.where(smoothed.isnull(), 0, xr.where(smoothed>=0.5, 2, 1)).astype('uint8').expand_dims(time=[date])\n",
    "#             print(smoothed.head)\n",
    "            \n",
    "            # save this to list\n",
    "            all_smoothed.append(smoothed)\n",
    "#             \n",
    "        # now at the end concat them all together, sort by date\n",
    "        snow2 = xr.concat(all_smoothed, dim='time').sortby('time')\n",
    "#         print(snow2.head)\n",
    "        \n",
    "#         fig,axs=plt.subplots(1,2, figsize=(14,5))\n",
    "#         snow['class'].mean(dim='time').plot(ax=axs[0])\n",
    "#         snow2.mean(dim='time').plot(ax=axs[1])\n",
    "#         continue\n",
    "\n",
    "        ### save\n",
    "        path_save = os.path.join(folder_save, 'Daily AAs', f\"S2_{rgiid}_{y}_daily_AAs_smoothed.nc\")\n",
    "        save_xr = snow2.sel(time=slice(f\"{y}-01-01\", f\"{y}-12-31\")).astype('uint8').rename('class')\n",
    "\n",
    "        # specify compression/encoding\n",
    "        encoding = {\"class\":{\"zlib\": True}}#, \"spatial_ref\":{\"zlib\": False}}\n",
    "\n",
    "        # save\n",
    "        save_xr.to_netcdf(path_save, encoding=encoding)\n",
    "    \n",
    "    print(\"Smoothed daily AAs made\")\n",
    "#         print(snow.head)\n",
    "#         print(snow2.head)\n",
    "        \n",
    "#         fig,axs=plt.subplots(1,2, figsize=(14,5))\n",
    "#         snow['class'].mean(dim='time').plot(ax=axs[0])\n",
    "#         snow2['class'].mean(dim='time').plot(ax=axs[1])\n",
    "#         continue\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
